from typing import Dict, List, Union

from catalyst.core import MetricCallback
import torch
from torch import nn
from torch.nn import functional as F



class SmoothingLossCallback(MetricCallback):
    """
    CosineLossCallback
    This callback is calculating cosine loss between hidden states
    of the two hugging face transformers models.
    """

    def __init__(
        self,
        input_key: Union[str, List[str], Dict[str, str]] = None,
        output_key: Union[str, List[str], Dict[str, str]] = None,
        prefix: str = "label_smooth_loss",
        multiplier: float = 1.0,
        **metric_kwargs,
    ):
        """
        Args:
            input_key (Union[str, List[str], Dict[str, str]]): key/list/dict
                of keys that takes values from the input dictionary
                If '__all__', the whole input will be passed to the criterion
                If None, empty dict will be passed to the criterion.
            output_key (Union[str, List[str], Dict[str, str]]): key/list/dict
                of keys that takes values from the input dictionary
                If '__all__', the whole output will be passed to the criterion
                If None, empty dict will be passed to the criterion.
            prefix (str): prefix for metrics and output key for loss
                in ``state.batch_metrics`` dictionary
            criterion_key (str): A key to take a criterion in case
                there are several of them and they are in a dictionary format.
            multiplier (float): scale factor for the output loss.
        """
        if output_key is None:
            output_key = [
                "target",
                "s_logits",
            ]
        super().__init__(
            prefix=prefix,
            input_key=input_key,
            output_key=output_key,
            multiplier=multiplier,
            metric_fn=self.metric_fn,
            **metric_kwargs,
        )

    def metric_fn(
        self,
        target: torch.Tensor,
        s_logits: torch.Tensor,
    ) -> torch.Tensor:
        """
        Computes label smoothing loss on given hidden states
        Args:
            labes: tensor from dataset labels
            s_logits: tensor from student model
        Returns:
            label smothing loss
        """
        epsilon = 0.1
        ignore_index = 0
        lprobs = F.log_softmax(s_logits, dim=-1)
        if target.dim() == lprobs.dim() - 1:
         target = target.unsqueeze(-1)
        nll_loss = -lprobs.gather(dim=-1, index=target)
        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)
        if ignore_index is not None:
          pad_mask = target.eq(ignore_index)
          nll_loss.masked_fill_(pad_mask, 0)
          smooth_loss.masked_fill_(pad_mask, 0)
        else:
          nll_loss = nll_loss.squeeze(-1)
          smooth_loss = smooth_loss.squeeze(-1)

        nll_loss = nll_loss.sum()  # mean()? Scared to break other math.
        smooth_loss = smooth_loss.sum()
        eps_i = epsilon / lprobs.size(-1)
        loss = (1 - epsilon) * nll_loss + eps_i * smooth_loss 
        return loss
