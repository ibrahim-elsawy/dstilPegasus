{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Copy of xsum-pegasusstudent_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93690b410bfa4f9b9dcde671073c9a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_77094ff4102d42f3a978bc7218efe6fb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cb5cfa8733dc4d5898568b038d0b141f",
              "IPY_MODEL_6efeda9398244583866f9475e934f5ef"
            ]
          }
        },
        "77094ff4102d42f3a978bc7218efe6fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb5cfa8733dc4d5898568b038d0b141f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a3161366b8ed466c98a8065c35ca625d",
            "_dom_classes": [],
            "description": "Skipping the first batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 6496,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 6496,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b98bbbbae55e4cb6981000126cfd37c7"
          }
        },
        "6efeda9398244583866f9475e934f5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82454449b0544bd6954e21999db8d37c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6496/6496 [01:00&lt;00:00, 107.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e6c23327a77048ddada550d360dd6f26"
          }
        },
        "a3161366b8ed466c98a8065c35ca625d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b98bbbbae55e4cb6981000126cfd37c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82454449b0544bd6954e21999db8d37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e6c23327a77048ddada550d360dd6f26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pC-ZVCezTyJ",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:10:24.196956Z",
          "iopub.execute_input": "2021-06-04T17:10:24.197341Z",
          "iopub.status.idle": "2021-06-04T17:10:38.741200Z",
          "shell.execute_reply.started": "2021-06-04T17:10:24.197306Z",
          "shell.execute_reply": "2021-06-04T17:10:38.739681Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e79a330-fc5e-45ba-9753-6c4d8ef20be7"
      },
      "source": [
        "! pip install datasets transformers rouge-score nltk wandb sentencepiece "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/a2/d4e1024c891506e1cee8f9d719d20831bac31cb5b7416983c4d2f65a6287/datasets-1.8.0-py3-none-any.whl (237kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 27.7MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 46.6MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b4/9d92953d8cddc8450c859be12e3dbdd4c7754fb8def94c28b3b351c6ee4e/wandb-0.10.32-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 47.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/d2/d05466997f7751a2c06a7a416b7d1f131d765f7916698d3fdcb3a4d037e5/fsspec-2021.6.0-py3-none-any.whl (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/e3/fb7b6aefaf0fc7b792cebbbd590b1895c022ab0ff27f389e1019c6f2e68a/huggingface_hub-0.0.10-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 56.7MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 57.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (57.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=5974da35c2d0629dc3796ed1601defe2c938eecf1c3f1a9d5b4bc49ae456766c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=5caec308ede8d5db541561af4f918582c726aa1fb7c55a3a6ffd0ee61e0db117\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "\u001b[31mERROR: transformers 4.6.1 has requirement huggingface-hub==0.0.8, but you'll have huggingface-hub 0.0.10 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fsspec, xxhash, huggingface-hub, datasets, tokenizers, sacremoses, transformers, rouge-score, subprocess32, docker-pycreds, configparser, sentry-sdk, smmap, gitdb, GitPython, shortuuid, pathtools, wandb, sentencepiece\n",
            "Successfully installed GitPython-3.1.17 configparser-5.0.2 datasets-1.8.0 docker-pycreds-0.4.0 fsspec-2021.6.0 gitdb-4.0.7 huggingface-hub-0.0.10 pathtools-0.1.2 rouge-score-0.0.4 sacremoses-0.0.45 sentencepiece-0.1.95 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tokenizers-0.10.3 transformers-4.6.1 wandb-0.10.32 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecUCYR3H_IQ",
        "trusted": true
      },
      "source": [
        "#!git clone https://github.com/huggingface/transformers\n",
        "#%cd transformers\n",
        "#!pip install .\n",
        "\n",
        "\n",
        "#!pip install -r ./transformers/examples/pytorch/summarization/requirements.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4tnxQoRogVV"
      },
      "source": [
        "# Install env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMXQ-moaoayT"
      },
      "source": [
        "### Install triton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4b2CBehhQX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d175fb8-3caf-4c1d-8fca-1b7755f5af63"
      },
      "source": [
        "!apt-get install llvm-9-dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  binfmt-support libffi-dev libllvm9 libpfm4 llvm-9 llvm-9-runtime\n",
            "  llvm-9-tools python-chardet python-pkg-resources python-pygments python-yaml\n",
            "  python3-pkg-resources python3-pygments python3-yaml\n",
            "Suggested packages:\n",
            "  llvm-9-doc python-setuptools ttf-bitstream-vera python3-setuptools\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev libllvm9 libpfm4 llvm-9 llvm-9-dev llvm-9-runtime\n",
            "  llvm-9-tools python-chardet python-pkg-resources python-pygments python-yaml\n",
            "  python3-pkg-resources python3-pygments python3-yaml\n",
            "0 upgraded, 15 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 48.8 MB of archives.\n",
            "After this operation, 287 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-yaml amd64 3.12-1build2 [109 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 binfmt-support amd64 2.1.8-2 [51.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libllvm9 amd64 1:9-2~ubuntu18.04.2 [14.8 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 llvm-9-runtime amd64 1:9-2~ubuntu18.04.2 [176 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpfm4 amd64 4.9.0-2 [225 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 llvm-9 amd64 1:9-2~ubuntu18.04.2 [4,874 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libffi-dev amd64 3.2.1-8 [156 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-pygments all 2.2.0+dfsg-1ubuntu0.2 [576 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-yaml amd64 3.12-1build2 [115 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-pygments all 2.2.0+dfsg-1ubuntu0.2 [574 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 llvm-9-tools amd64 1:9-2~ubuntu18.04.2 [250 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 llvm-9-dev amd64 1:9-2~ubuntu18.04.2 [26.6 MB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-chardet all 3.0.4-1 [80.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Fetched 48.8 MB in 2s (23.3 MB/s)\n",
            "Selecting previously unselected package python3-yaml.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python3-yaml_3.12-1build2_amd64.deb ...\n",
            "Unpacking python3-yaml (3.12-1build2) ...\n",
            "Selecting previously unselected package binfmt-support.\n",
            "Preparing to unpack .../01-binfmt-support_2.1.8-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.1.8-2) ...\n",
            "Selecting previously unselected package libllvm9:amd64.\n",
            "Preparing to unpack .../02-libllvm9_1%3a9-2~ubuntu18.04.2_amd64.deb ...\n",
            "Unpacking libllvm9:amd64 (1:9-2~ubuntu18.04.2) ...\n",
            "Selecting previously unselected package llvm-9-runtime.\n",
            "Preparing to unpack .../03-llvm-9-runtime_1%3a9-2~ubuntu18.04.2_amd64.deb ...\n",
            "Unpacking llvm-9-runtime (1:9-2~ubuntu18.04.2) ...\n",
            "Selecting previously unselected package libpfm4:amd64.\n",
            "Preparing to unpack .../04-libpfm4_4.9.0-2_amd64.deb ...\n",
            "Unpacking libpfm4:amd64 (4.9.0-2) ...\n",
            "Selecting previously unselected package llvm-9.\n",
            "Preparing to unpack .../05-llvm-9_1%3a9-2~ubuntu18.04.2_amd64.deb ...\n",
            "Unpacking llvm-9 (1:9-2~ubuntu18.04.2) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../06-libffi-dev_3.2.1-8_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.2.1-8) ...\n",
            "Selecting previously unselected package python-pygments.\n",
            "Preparing to unpack .../07-python-pygments_2.2.0+dfsg-1ubuntu0.2_all.deb ...\n",
            "Unpacking python-pygments (2.2.0+dfsg-1ubuntu0.2) ...\n",
            "Selecting previously unselected package python-yaml.\n",
            "Preparing to unpack .../08-python-yaml_3.12-1build2_amd64.deb ...\n",
            "Unpacking python-yaml (3.12-1build2) ...\n",
            "Selecting previously unselected package python3-pygments.\n",
            "Preparing to unpack .../09-python3-pygments_2.2.0+dfsg-1ubuntu0.2_all.deb ...\n",
            "Unpacking python3-pygments (2.2.0+dfsg-1ubuntu0.2) ...\n",
            "Selecting previously unselected package llvm-9-tools.\n",
            "Preparing to unpack .../10-llvm-9-tools_1%3a9-2~ubuntu18.04.2_amd64.deb ...\n",
            "Unpacking llvm-9-tools (1:9-2~ubuntu18.04.2) ...\n",
            "Selecting previously unselected package llvm-9-dev.\n",
            "Preparing to unpack .../11-llvm-9-dev_1%3a9-2~ubuntu18.04.2_amd64.deb ...\n",
            "Unpacking llvm-9-dev (1:9-2~ubuntu18.04.2) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../12-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../13-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../14-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python3-yaml (3.12-1build2) ...\n",
            "Setting up binfmt-support (2.1.8-2) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service → /lib/systemd/system/binfmt-support.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up python-yaml (3.12-1build2) ...\n",
            "Setting up libffi-dev:amd64 (3.2.1-8) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up libpfm4:amd64 (4.9.0-2) ...\n",
            "Setting up python-pygments (2.2.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up libllvm9:amd64 (1:9-2~ubuntu18.04.2) ...\n",
            "Setting up python3-pygments (2.2.0+dfsg-1ubuntu0.2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up llvm-9-runtime (1:9-2~ubuntu18.04.2) ...\n",
            "Setting up llvm-9-tools (1:9-2~ubuntu18.04.2) ...\n",
            "Setting up llvm-9 (1:9-2~ubuntu18.04.2) ...\n",
            "Setting up llvm-9-dev (1:9-2~ubuntu18.04.2) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.47) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbWxEvnN02bY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c74b5b-6f3a-43f6-b02a-d88b8212d7a2"
      },
      "source": [
        "!pip install cpufeature"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cpufeature\n",
            "  Downloading https://files.pythonhosted.org/packages/78/7e/ad2c48300508047935a06c6ae09e30504626aad2b75b17bebfbcf08546b5/cpufeature-0.2.0-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Installing collected packages: cpufeature\n",
            "Successfully installed cpufeature-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euv2A1weyZ5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a1ac94-2572-4f95-cd58-6658db174d6d"
      },
      "source": [
        "!pip install triton==0.2.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting triton==0.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/3b/7b9c8619e9eb4e0eb3cf9e01dccd1b85161f5d9e452d7fbf3aca23a62675/triton-0.2.3.tar.gz (943kB)\n",
            "\r\u001b[K     |▍                               | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 29.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 31.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 33.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 34.8MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 35.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 35.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 33.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 92kB 34.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102kB 34.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112kB 34.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122kB 34.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133kB 34.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 174kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 184kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 204kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 235kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 256kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 276kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 337kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 348kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 358kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 368kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 378kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 419kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 430kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 440kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 450kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 460kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 471kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 512kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 522kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 542kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 552kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 583kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 593kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 604kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 614kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 624kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 634kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 645kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 655kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 665kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 675kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 686kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 696kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 706kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 716kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 727kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 737kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 747kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 757kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 778kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 788kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 808kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 819kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 829kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 839kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 849kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 860kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 870kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 880kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 890kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 901kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 911kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 921kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 931kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 942kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 952kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from triton==0.2.3) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from triton==0.2.3) (1.8.1+cu101)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from triton==0.2.3) (1.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->triton==0.2.3) (3.7.4.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->triton==0.2.3) (1.2.1)\n",
            "Building wheels for collected packages: triton\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IId2GBmCod9A"
      },
      "source": [
        "### Install DeepSpeed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uz1X8QcgGs6"
      },
      "source": [
        "!pip install libaio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmx7xwp_Kmz6"
      },
      "source": [
        "!rm /usr/local/cuda\n",
        "!ln -s /usr/local/cuda-10.1 /usr/local/cuda\n",
        "! apt install libaio-dev\n",
        "!DS_BUILD_TRANSFORMER=1 DS_BUILD_STOCHASTIC_TRANSFORMER=1 DS_BUILD_UTILS=1 \n",
        "!DS_BUILD_CPU_ADAM=1 DS_BUILD_SPARSE_ATTN=1 pip install transformers[deepspeed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47RnjoAJolAc"
      },
      "source": [
        "#### Test installation: we should have the following output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRR8wi7kK3dO"
      },
      "source": [
        "!ds_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbNKoC8mo0_S"
      },
      "source": [
        "### Download repo and install other libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t43yH5k1jtZZ"
      },
      "source": [
        "!git clone  https://github.com/sberbank-ai/ru-gpts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJLYfiOitYNx"
      },
      "source": [
        "!pip install natsort"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQRgzyCUYg0P"
      },
      "source": [
        "# Play\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVi_PCEkOuB1",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:10:15.924750Z",
          "iopub.execute_input": "2021-06-04T17:10:15.925207Z",
          "iopub.status.idle": "2021-06-04T17:10:16.684831Z",
          "shell.execute_reply.started": "2021-06-04T17:10:15.925154Z",
          "shell.execute_reply": "2021-06-04T17:10:16.683513Z"
        },
        "trusted": true
      },
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66hiCqECzbId",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:10:16.855924Z",
          "iopub.execute_input": "2021-06-04T17:10:16.856250Z",
          "iopub.status.idle": "2021-06-04T17:10:16.863568Z",
          "shell.execute_reply.started": "2021-06-04T17:10:16.856216Z",
          "shell.execute_reply": "2021-06-04T17:10:16.860592Z"
        },
        "trusted": true
      },
      "source": [
        "student_number = 4\n",
        "data_name = 'xsum'\n",
        "#model_name = f'google/pegasus-{data_name}'\n",
        "model_name = 'tuner007/pegasus_paraphrase'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bmiy0e21_jt",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:36.976131Z",
          "iopub.status.idle": "2021-06-04T17:06:36.976525Z"
        },
        "trusted": true
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#directory = f'/content/drive/MyDrive/GP/dataset'\n",
        "#if not os.path.exists(directory):\n",
        "#    os.makedirs(directory)\n",
        "\n",
        "PATH = f'/content/drive/MyDrive/GP/student_{data_name}_{student_number}'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreSlFmlIrIm",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:08:52.446350Z",
          "iopub.execute_input": "2021-06-04T17:08:52.446735Z",
          "iopub.status.idle": "2021-06-04T17:08:52.488063Z",
          "shell.execute_reply.started": "2021-06-04T17:08:52.446704Z",
          "shell.execute_reply": "2021-06-04T17:08:52.483180Z"
        },
        "trusted": true
      },
      "source": [
        "from datasets import load_dataset, load_metric, load_from_disk\n",
        "\n",
        "#tokenized_datasets = load_from_disk(f'{PATH}/../dataset/{data_name}')\n",
        "tokenized_datasets = load_from_disk('/content/drive/MyDrive/GP/dataset/xsum')\n",
        "#raw_datasets = load_dataset(data_name)\n",
        "metric = load_metric(\"rouge\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFIT8cHuL12a"
      },
      "source": [
        "tokens = tokenized_datasets.filter(lambda example, indice: indice < 1001, with_indices=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSw5EYbwM1TU"
      },
      "source": [
        "tokenized_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VjoMVMXz0YI",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:36.978802Z",
          "iopub.status.idle": "2021-06-04T17:06:36.979261Z"
        },
        "trusted": true
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbO0tLnW0Zn8"
      },
      "source": [
        "# Model Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYAsu_rJ0RKJ",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:36.996047Z",
          "iopub.status.idle": "2021-06-04T17:06:36.996624Z"
        },
        "trusted": true
      },
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, Trainer\n",
        "\n",
        "max_input_length = 1024\n",
        "max_target_length = 60\n",
        "\n",
        "teacher = AutoModelForSeq2SeqLM.from_pretrained(model_name, max_length = max_input_length, max_position_embeddings=max_input_length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hu5HQ3A0qak",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:36.997903Z",
          "iopub.status.idle": "2021-06-04T17:06:36.998465Z"
        },
        "trusted": true
      },
      "source": [
        "import datasets\n",
        "import warnings\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional, Tuple, List, Union\n",
        "from transformers import PegasusModel, PegasusConfig, PegasusForConditionalGeneration\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PreTrainedModel\n",
        "from transformers import SummarizationPipeline\n",
        "students_config_book = {\n",
        "    '2': PegasusConfig(encoder_layers=2, decoder_layers=2),\n",
        "    '4': PegasusConfig(encoder_layers=4, decoder_layers=4),\n",
        "    '6': PegasusConfig(encoder_layers=6, decoder_layers=6),\n",
        "    '8': PegasusConfig(encoder_layers=8, decoder_layers=8),\n",
        "    '10': PegasusConfig(encoder_layers=10, decoder_layers=10),\n",
        "    '12': PegasusConfig(encoder_layers=12, decoder_layers=12),\n",
        "    '16': PegasusConfig(encoder_layers=16, decoder_layers=16)\n",
        "}\n",
        "\n",
        "\n",
        "LAYERS_TO_COPY = {\n",
        "    16: {  # maps  num layers in student -> which teacher layers to copy\n",
        "        1: [0],\n",
        "        2: [0, 15],\n",
        "        3: [0, 8, 15],\n",
        "        4: [0, 5, 10, 15],\n",
        "        6: [0, 3, 6, 9, 12, 15],\n",
        "        8: [0, 2, 4, 6, 8, 10, 12, 15],\n",
        "        9: [0, 1, 3, 5, 7, 9, 11, 13, 15],\n",
        "        12: [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 13, 15],\n",
        "        16: list(range(16)),\n",
        "    },\n",
        "}\n",
        "LAYERS_TO_SUPERVISE = {\n",
        "    # maps  num layers in student -> which teacher layers to copy.\n",
        "    16: {1: [15], 4: [4, 9, 12, 15], 8: [1, 3, 5, 7, 9, 11, 13, 15]},\n",
        "}\n",
        "\n",
        "\n",
        "def copy_layers(src_layers: nn.ModuleList, dest_layers: nn.ModuleList, layers_to_copy) -> None:\n",
        "    layers_to_copy = nn.ModuleList([src_layers[i] for i in layers_to_copy])\n",
        "    assert len(dest_layers) == len(\n",
        "        layers_to_copy), f\"{len(dest_layers)} != {len(layers_to_copy)}\"\n",
        "    dest_layers.load_state_dict(layers_to_copy.state_dict())\n",
        "\n",
        "# Copied from transformers.models.bart.modeling_bart.shift_tokens_right\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n",
        "\n",
        "def pick_layers_to_copy(n_student, n_teacher):\n",
        "    try:\n",
        "        val = LAYERS_TO_COPY[n_teacher][n_student]\n",
        "        return val\n",
        "    except KeyError:\n",
        "        if n_student != n_teacher:\n",
        "            warnings.warn(\n",
        "                f\"no hardcoded layers to copy for teacher {n_teacher} -> student {n_student}, defaulting to first {n_student}\"\n",
        "            )\n",
        "        return list(range(n_student))\n",
        "\n",
        "\n",
        "def get_layers_to_supervise(n_student, n_teacher) -> List[int]:\n",
        "    \"\"\"Used or the --supervise_forward kwarg\"\"\"\n",
        "    if n_student > n_teacher:\n",
        "        raise ValueError(\n",
        "            f\"Cannot perform intermediate supervision for student {n_student} > teacher {n_teacher}\")\n",
        "    elif n_teacher == n_student:\n",
        "        return list(range(n_teacher))\n",
        "    elif n_student == 1:\n",
        "        return [n_teacher - 1]\n",
        "    else:\n",
        "        return LAYERS_TO_SUPERVISE[n_teacher][n_student]\n",
        "\n",
        "\n",
        "def create_student_with_configuration(teacher,\n",
        "                                      e=None,\n",
        "                                      d=None,\n",
        "                                      copy_first_teacher_layers = False,\n",
        "                                      save_path='./student'):\n",
        "\n",
        "    teacher.eval()\n",
        "    teacher_e, teacher_d = teacher.config.encoder_layers, teacher.config.decoder_layers\n",
        "    init_kwargs = teacher.config.to_diff_dict()\n",
        "    if e is None:\n",
        "        e = teacher_e\n",
        "    if d is None:\n",
        "        d = teacher_d\n",
        "    init_kwargs.update({\"encoder_layers\": e, \"decoder_layers\": d})\n",
        "    student_cfg = teacher.config_class(**init_kwargs)\n",
        "    student = AutoModelForSeq2SeqLM.from_config(student_cfg)\n",
        "    # Start by copying the full teacher state dict this will copy the first N teacher layers to the student.\n",
        "    info = student.load_state_dict(teacher.state_dict(), strict=False)\n",
        "    # every student key should have a teacher keys.\n",
        "    assert info.missing_keys == [], info.missing_keys\n",
        "\n",
        "    if copy_first_teacher_layers:  # Our copying is done. We just log and save\n",
        "        e_layers_to_copy, d_layers_to_copy = list(range(e)), list(range(d))\n",
        "        student.save_pretrained(save_path)\n",
        "        return student, e_layers_to_copy, d_layers_to_copy\n",
        "\n",
        "    # Decide which layers of the teacher to copy. Not exactly alternating -- we try to keep first and last layer.\n",
        "    e_layers_to_copy: List[int] = pick_layers_to_copy(e, teacher_e)\n",
        "    d_layers_to_copy: List[int] = pick_layers_to_copy(d, teacher_d)\n",
        "\n",
        "    copy_layers(teacher.model.encoder.layers,\n",
        "                student.model.encoder.layers, e_layers_to_copy)\n",
        "    copy_layers(teacher.model.decoder.layers,\n",
        "                student.model.decoder.layers, d_layers_to_copy)\n",
        "\n",
        "    student.config.init_metadata = dict(\n",
        "        teacher_type=teacher.config.model_type,\n",
        "        copied_encoder_layers=e_layers_to_copy,\n",
        "        copied_decoder_layers=d_layers_to_copy,\n",
        "    )\n",
        "    #student.save_pretrained(save_path)\n",
        "    # Save information about copying for easier reproducibility\n",
        "\n",
        "    return student\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxZ5zUJe0t_N",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:36.999623Z",
          "iopub.status.idle": "2021-06-04T17:06:37.000156Z"
        },
        "trusted": true
      },
      "source": [
        "student = create_student_with_configuration(\n",
        "                                      teacher,\n",
        "                                      e=16,\n",
        "                                      d=student_number,\n",
        "                                      copy_first_teacher_layers = False,\n",
        "                                      save_path=PATH).to('cuda')\n",
        "\n",
        "del teacher\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKjUJ0Gj0t6E",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:37.001382Z",
          "iopub.status.idle": "2021-06-04T17:06:37.001915Z"
        },
        "trusted": true
      },
      "source": [
        "for param in student.model.shared.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in student.model.encoder.parameters():\n",
        "  param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD3I-ELPg-6H"
      },
      "source": [
        "#before\n",
        "student.model.decoder.layers[0].final_layer_norm.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsAiZClNyWzQ"
      },
      "source": [
        "#after\n",
        "student.model.decoder.layers[0].final_layer_norm.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTJ5upin04_-"
      },
      "source": [
        "# The Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grkpril21OGF",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:37.003068Z",
          "iopub.status.idle": "2021-06-04T17:06:37.003755Z"
        },
        "trusted": true
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        " \n",
        "import numpy as np\n",
        " \n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8qnEhQL5C2",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:37.005164Z",
          "iopub.status.idle": "2021-06-04T17:06:37.005725Z"
        },
        "trusted": true
      },
      "source": [
        "\n",
        "class MyTrainer(Seq2SeqTrainer):\n",
        "    \n",
        "    def shift_tokens_right(self,input_ids: torch.Tensor, pad_token_id = 0, decoder_start_token_id = 0):\n",
        "      \"\"\"\n",
        "      Shift input ids one token to the right.\n",
        "      \"\"\"\n",
        "      shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "      shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "      shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "      assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
        "      # replace possible -100 values in labels by `pad_token_id`\n",
        "      shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "      return shifted_input_ids\n",
        "    \n",
        "    \n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs[\"labels\"]\n",
        "        pad_token_id = tokenizer.pad_token_id\n",
        "        decoder_input_ids = self.shift_tokens_right(labels, pad_token_id)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        #print(f\"input: {input_ids}, masks: {attention_mask}\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[\"logits\"]\n",
        "        #print(f'logits: {logits}, labels: {decoder_input_ids}')\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.view(-1, logits.shape[-1]),labels.view(-1))\n",
        "        #print(loss)\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obU0YBuJLhg7"
      },
      "source": [
        "%%bash\n",
        "cat <<\"EOT\" > ds_config_zero2.json\n",
        "{\n",
        "    \"fp16\":{\n",
        "        \"enabled\":false,\n",
        "        \"hysteresis\":2,\n",
        "        \"initial_scale_power\":32,\n",
        "        \"loss_scale\":0,\n",
        "        \"loss_scale_window\":1000,\n",
        "        \"min_loss_scale\":1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"amp\": {\n",
        "      \"enabled\":false,\n",
        "      \"opt_level\": \"O1\"\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "      \"stage\": 2,\n",
        "      \"offload_optimizer\": {\n",
        "          \"device\": \"cpu\",\n",
        "          \"pin_memory\": true\n",
        "      },\n",
        "      \"allgather_partitions\": true,\n",
        "      \"allgather_bucket_size\": 2e8,\n",
        "      \"reduce_scatter\": true,\n",
        "      \"reduce_bucket_size\": 2e8,\n",
        "      \"overlap_comm\": true,\n",
        "      \"contiguous_gradients\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 500,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsb37ME5MNNf"
      },
      "source": [
        "import os\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
        "os.environ['RANK'] = \"0\"\n",
        "os.environ['LOCAL_RANK'] = \"0\"\n",
        "os.environ['WORLD_SIZE'] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z9MtTIY02x2",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:37.006930Z",
          "iopub.status.idle": "2021-06-04T17:06:37.007509Z"
        },
        "trusted": true
      },
      "source": [
        "batch_size = 16\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=f'{PATH}/output',\n",
        "    evaluation_strategy = \"steps\",\n",
        "    save_strategy = 'epoch',\n",
        "    eval_steps = 500,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size-8,\n",
        "    gradient_accumulation_steps = 16,\n",
        "    eval_accumulation_steps = 16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    #weight_decay=0,\n",
        "    logging_dir=f'{PATH}/logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    load_best_model_at_end = True,\n",
        "    #adafactor = True,\n",
        "    #fp16=True,\n",
        "    deepspeed = \"ds_config_zero2.json\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=student)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsviUqGN1cC-",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:37.813762Z",
          "iopub.execute_input": "2021-06-04T17:06:37.814083Z",
          "iopub.status.idle": "2021-06-04T17:06:44.472121Z",
          "shell.execute_reply.started": "2021-06-04T17:06:37.814053Z",
          "shell.execute_reply": "2021-06-04T17:06:44.470284Z"
        },
        "trusted": true
      },
      "source": [
        "from transformers import Adafactor, EarlyStoppingCallback\n",
        "#optimizer = Adafactor(student.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "\n",
        "trainer = MyTrainer(\n",
        "    student,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokens[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    #optimizers= (optimizer,_)\n",
        "    callbacks = [EarlyStoppingCallback]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n42eQdZo1jC9",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:44.472991Z",
          "iopub.status.idle": "2021-06-04T17:06:44.473375Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "93690b410bfa4f9b9dcde671073c9a25",
            "77094ff4102d42f3a978bc7218efe6fb",
            "cb5cfa8733dc4d5898568b038d0b141f",
            "6efeda9398244583866f9475e934f5ef",
            "a3161366b8ed466c98a8065c35ca625d",
            "b98bbbbae55e4cb6981000126cfd37c7",
            "82454449b0544bd6954e21999db8d37c",
            "e6c23327a77048ddada550d360dd6f26"
          ]
        },
        "outputId": "2f98147e-55d9-4aaa-ea29-c7bb50e7c04d"
      },
      "source": [
        "trainer.train('/content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2000')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-11 15:40:00,678] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.4.0, git-hash=unknown, git-branch=unknown\n",
            "[2021-06-11 15:40:00,693] [INFO] [utils.py:13:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/11/2021 15:40:00 - INFO - root -   Added key: store_based_barrier_key:2 to store for rank: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-06-11 15:40:00,925] [INFO] [engine.py:173:__init__] DeepSpeed Flops Profiler Enabled: False\n",
            "[2021-06-11 15:40:01,443] [INFO] [engine.py:693:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2021-06-11 15:40:01,445] [INFO] [engine.py:697:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2021-06-11 15:40:01,453] [INFO] [logging.py:60:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
            "[2021-06-11 15:40:01,460] [INFO] [stage2.py:105:__init__] Reduce bucket size 200000000.0\n",
            "[2021-06-11 15:40:01,461] [INFO] [stage2.py:106:__init__] Allgather bucket size 200000000.0\n",
            "[2021-06-11 15:40:01,462] [INFO] [stage2.py:107:__init__] CPU Offload: True\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/utils...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 19.64672350883484 seconds\n",
            "[2021-06-11 15:40:24,539] [INFO] [stage2.py:409:__init__] optimizer state initialized\n",
            "[2021-06-11 15:40:24,540] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2021-06-11 15:40:24,543] [INFO] [engine.py:505:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-06-11 15:40:24,547] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7feea6b33a50>\n",
            "[2021-06-11 15:40:24,551] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n",
            "[2021-06-11 15:40:24,556] [INFO] [config.py:900:print] DeepSpeedEngine configuration:\n",
            "[2021-06-11 15:40:24,560] [INFO] [config.py:904:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2021-06-11 15:40:24,562] [INFO] [config.py:904:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2021-06-11 15:40:24,564] [INFO] [config.py:904:print]   allreduce_always_fp32 ........ False\n",
            "[2021-06-11 15:40:24,567] [INFO] [config.py:904:print]   amp_enabled .................. False\n",
            "[2021-06-11 15:40:24,571] [INFO] [config.py:904:print]   amp_params ................... {'opt_level': 'O1'}\n",
            "[2021-06-11 15:40:24,573] [INFO] [config.py:904:print]   checkpoint_tag_validation_enabled  True\n",
            "[2021-06-11 15:40:24,576] [INFO] [config.py:904:print]   checkpoint_tag_validation_fail  False\n",
            "[2021-06-11 15:40:24,578] [INFO] [config.py:904:print]   disable_allgather ............ False\n",
            "[2021-06-11 15:40:24,581] [INFO] [config.py:904:print]   dump_state ................... False\n",
            "[2021-06-11 15:40:24,584] [INFO] [config.py:904:print]   dynamic_loss_scale_args ...... None\n",
            "[2021-06-11 15:40:24,586] [INFO] [config.py:904:print]   eigenvalue_enabled ........... False\n",
            "[2021-06-11 15:40:24,589] [INFO] [config.py:904:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2021-06-11 15:40:24,591] [INFO] [config.py:904:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2021-06-11 15:40:24,595] [INFO] [config.py:904:print]   eigenvalue_layer_num ......... 0\n",
            "[2021-06-11 15:40:24,597] [INFO] [config.py:904:print]   eigenvalue_max_iter .......... 100\n",
            "[2021-06-11 15:40:24,599] [INFO] [config.py:904:print]   eigenvalue_stability ......... 1e-06\n",
            "[2021-06-11 15:40:24,601] [INFO] [config.py:904:print]   eigenvalue_tol ............... 0.01\n",
            "[2021-06-11 15:40:24,603] [INFO] [config.py:904:print]   eigenvalue_verbose ........... False\n",
            "[2021-06-11 15:40:24,606] [INFO] [config.py:904:print]   elasticity_enabled ........... False\n",
            "[2021-06-11 15:40:24,611] [INFO] [config.py:904:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2021-06-11 15:40:24,612] [INFO] [config.py:904:print]   fp16_enabled ................. False\n",
            "[2021-06-11 15:40:24,614] [INFO] [config.py:904:print]   fp16_mixed_quantize .......... False\n",
            "[2021-06-11 15:40:24,615] [INFO] [config.py:904:print]   global_rank .................. 0\n",
            "[2021-06-11 15:40:24,616] [INFO] [config.py:904:print]   gradient_accumulation_steps .. 16\n",
            "[2021-06-11 15:40:24,618] [INFO] [config.py:904:print]   gradient_clipping ............ 1.0\n",
            "[2021-06-11 15:40:24,620] [INFO] [config.py:904:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-06-11 15:40:24,622] [INFO] [config.py:904:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2021-06-11 15:40:24,631] [INFO] [config.py:904:print]   loss_scale ................... 0\n",
            "[2021-06-11 15:40:24,635] [INFO] [config.py:904:print]   memory_breakdown ............. False\n",
            "[2021-06-11 15:40:24,636] [INFO] [config.py:904:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-06-11 15:40:24,638] [INFO] [config.py:904:print]   optimizer_name ............... adamw\n",
            "[2021-06-11 15:40:24,640] [INFO] [config.py:904:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
            "[2021-06-11 15:40:24,644] [INFO] [config.py:904:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-06-11 15:40:24,645] [INFO] [config.py:904:print]   pld_enabled .................. False\n",
            "[2021-06-11 15:40:24,648] [INFO] [config.py:904:print]   pld_params ................... False\n",
            "[2021-06-11 15:40:24,649] [INFO] [config.py:904:print]   prescale_gradients ........... False\n",
            "[2021-06-11 15:40:24,652] [INFO] [config.py:904:print]   quantize_change_rate ......... 0.001\n",
            "[2021-06-11 15:40:24,654] [INFO] [config.py:904:print]   quantize_groups .............. 1\n",
            "[2021-06-11 15:40:24,655] [INFO] [config.py:904:print]   quantize_offset .............. 1000\n",
            "[2021-06-11 15:40:24,658] [INFO] [config.py:904:print]   quantize_period .............. 1000\n",
            "[2021-06-11 15:40:24,659] [INFO] [config.py:904:print]   quantize_rounding ............ 0\n",
            "[2021-06-11 15:40:24,661] [INFO] [config.py:904:print]   quantize_start_bits .......... 16\n",
            "[2021-06-11 15:40:24,663] [INFO] [config.py:904:print]   quantize_target_bits ......... 8\n",
            "[2021-06-11 15:40:24,666] [INFO] [config.py:904:print]   quantize_training_enabled .... False\n",
            "[2021-06-11 15:40:24,667] [INFO] [config.py:904:print]   quantize_type ................ 0\n",
            "[2021-06-11 15:40:24,670] [INFO] [config.py:904:print]   quantize_verbose ............. False\n",
            "[2021-06-11 15:40:24,672] [INFO] [config.py:904:print]   scheduler_name ............... WarmupLR\n",
            "[2021-06-11 15:40:24,673] [INFO] [config.py:904:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 1e-05, 'warmup_num_steps': 500}\n",
            "[2021-06-11 15:40:24,677] [INFO] [config.py:904:print]   sparse_attention ............. None\n",
            "[2021-06-11 15:40:24,678] [INFO] [config.py:904:print]   sparse_gradients_enabled ..... False\n",
            "[2021-06-11 15:40:24,680] [INFO] [config.py:904:print]   steps_per_print .............. 500\n",
            "[2021-06-11 15:40:24,682] [INFO] [config.py:904:print]   tensorboard_enabled .......... False\n",
            "[2021-06-11 15:40:24,685] [INFO] [config.py:904:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-06-11 15:40:24,687] [INFO] [config.py:904:print]   tensorboard_output_path ...... \n",
            "[2021-06-11 15:40:24,689] [INFO] [config.py:904:print]   train_batch_size ............. 256\n",
            "[2021-06-11 15:40:24,690] [INFO] [config.py:904:print]   train_micro_batch_size_per_gpu  16\n",
            "[2021-06-11 15:40:24,693] [INFO] [config.py:904:print]   use_quantizer_kernel ......... False\n",
            "[2021-06-11 15:40:24,694] [INFO] [config.py:904:print]   wall_clock_breakdown ......... False\n",
            "[2021-06-11 15:40:24,696] [INFO] [config.py:904:print]   world_size ................... 1\n",
            "[2021-06-11 15:40:24,699] [INFO] [config.py:904:print]   zero_allow_untested_optimizer  False\n",
            "[2021-06-11 15:40:24,701] [INFO] [config.py:904:print]   zero_config .................. {\n",
            "    \"stage\": 2, \n",
            "    \"contiguous_gradients\": true, \n",
            "    \"reduce_scatter\": true, \n",
            "    \"reduce_bucket_size\": 2.000000e+08, \n",
            "    \"allgather_partitions\": true, \n",
            "    \"allgather_bucket_size\": 2.000000e+08, \n",
            "    \"overlap_comm\": true, \n",
            "    \"load_from_fp32_weights\": true, \n",
            "    \"elastic_checkpoint\": true, \n",
            "    \"offload_param\": null, \n",
            "    \"offload_optimizer\": {\n",
            "        \"device\": \"cpu\", \n",
            "        \"nvme_path\": null, \n",
            "        \"buffer_count\": 4, \n",
            "        \"pin_memory\": true, \n",
            "        \"pipeline_read\": false, \n",
            "        \"pipeline_write\": false, \n",
            "        \"fast_init\": false, \n",
            "        \"pipeline\": false\n",
            "    }, \n",
            "    \"sub_group_size\": 1.000000e+12, \n",
            "    \"prefetch_bucket_size\": 5.000000e+07, \n",
            "    \"param_persistence_threshold\": 1.000000e+05, \n",
            "    \"max_live_parameters\": 1.000000e+09, \n",
            "    \"max_reuse_distance\": 1.000000e+09, \n",
            "    \"gather_fp16_weights_on_model_save\": false, \n",
            "    \"ignore_unused_parameters\": true, \n",
            "    \"legacy_stage1\": false\n",
            "}\n",
            "[2021-06-11 15:40:24,704] [INFO] [config.py:904:print]   zero_enabled ................. True\n",
            "[2021-06-11 15:40:24,706] [INFO] [config.py:904:print]   zero_optimization_stage ...... 2\n",
            "[2021-06-11 15:40:24,708] [INFO] [config.py:911:print]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": false, \n",
            "        \"hysteresis\": 2, \n",
            "        \"initial_scale_power\": 32, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 1e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.01\n",
            "        }\n",
            "    }, \n",
            "    \"amp\": {\n",
            "        \"enabled\": false, \n",
            "        \"opt_level\": \"O1\"\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 1e-05, \n",
            "            \"warmup_num_steps\": 500\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 2.000000e+08, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 2.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 16, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": 500, \n",
            "    \"train_batch_size\": 256, \n",
            "    \"train_micro_batch_size_per_gpu\": 16, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0007627010345458984 seconds\n",
            "[2021-06-11 15:40:25,095] [INFO] [state_dict_factory.py:165:check_ckpt_list] checkpoint file list: ['/content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2000/global_step1203/mp_rank_00_model_states.pt']\n",
            "[2021-06-11 15:40:59,253] [INFO] [state_dict_factory.py:56:load] mp_world_size: 1, mp_rank: 0, module_key: auto\n",
            "[2021-06-11 15:40:59,260] [INFO] [state_dict_factory.py:85:load] rank: 0 loading checkpoint: /content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2000/global_step1203/mp_rank_00_model_states.pt\n",
            "successfully loaded 1 ZeRO state_dicts for rank 0\n",
            "loading 1 zero partition checkpoints for rank 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93690b410bfa4f9b9dcde671073c9a25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=6496.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.32<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">/content/drive/MyDrive/GP/student_xsum_4/output</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/hadywalied/huggingface\" target=\"_blank\">https://wandb.ai/hadywalied/huggingface</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/hadywalied/huggingface/runs/2az2k46j\" target=\"_blank\">https://wandb.ai/hadywalied/huggingface/runs/2az2k46j</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210611_154131-2az2k46j</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2580' max='3188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2580/3188 2:22:31 < 2:29:54, 0.07 it/s, Epoch 3.24/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>6.131500</td>\n",
              "      <td>5.691840</td>\n",
              "      <td>29.645300</td>\n",
              "      <td>9.929800</td>\n",
              "      <td>23.829100</td>\n",
              "      <td>23.819100</td>\n",
              "      <td>21.812200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-06-11 15:49:39,540] [INFO] [timer.py:160:stop] 0/500, SamplesPerSec=18.582311372347352\n",
            "[2021-06-11 15:56:57,337] [INFO] [timer.py:160:stop] 0/1000, SamplesPerSec=18.516574226680017\n",
            "[2021-06-11 16:04:15,055] [INFO] [timer.py:160:stop] 0/1500, SamplesPerSec=18.49506367651793\n",
            "[2021-06-11 16:11:32,473] [INFO] [timer.py:160:stop] 0/2000, SamplesPerSec=18.487094198768858\n",
            "[2021-06-11 16:18:49,537] [INFO] [timer.py:160:stop] 0/2500, SamplesPerSec=18.48510646609173\n",
            "[2021-06-11 16:26:06,847] [INFO] [timer.py:160:stop] 0/3000, SamplesPerSec=18.48220620850975\n",
            "[2021-06-11 16:33:24,063] [INFO] [timer.py:160:stop] 0/3500, SamplesPerSec=18.48042918875149\n",
            "[2021-06-11 16:40:41,188] [INFO] [timer.py:160:stop] 0/4000, SamplesPerSec=18.479593321275228\n",
            "[2021-06-11 16:47:58,237] [INFO] [timer.py:160:stop] 0/4500, SamplesPerSec=18.479709925598268\n",
            "[2021-06-11 16:51:38,542] [INFO] [logging.py:60:log_dist] [Rank 0] step=1500, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n",
            "[2021-06-11 16:55:15,144] [INFO] [timer.py:160:stop] 0/5000, SamplesPerSec=18.48012108171641\n",
            "[2021-06-11 17:02:32,279] [INFO] [timer.py:160:stop] 0/5500, SamplesPerSec=18.47945698389113\n",
            "[2021-06-11 17:09:49,898] [INFO] [timer.py:160:stop] 0/6000, SamplesPerSec=18.477350098889882\n",
            "[2021-06-11 17:13:43,911] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2391/global_step1594/mp_rank_00_model_states.pt\n",
            "[2021-06-11 17:14:41,997] [INFO] [engine.py:1938:_copy_recovery_script] creating recovery script /content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2391/zero_to_fp32.py\n",
            "[2021-06-11 17:14:42,117] [INFO] [engine.py:1951:_save_zero_checkpoint] zero checkpoint saved /content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2391/global_step1594/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2021-06-11 17:18:40,577] [INFO] [timer.py:160:stop] 0/6500, SamplesPerSec=18.477429350330073\n",
            "[2021-06-11 17:25:57,980] [INFO] [timer.py:160:stop] 0/7000, SamplesPerSec=18.478120931568093\n",
            "[2021-06-11 17:33:15,142] [INFO] [timer.py:160:stop] 0/7500, SamplesPerSec=18.478424777651938\n",
            "[2021-06-11 17:40:32,881] [INFO] [timer.py:160:stop] 0/8000, SamplesPerSec=18.477117298157257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/11/2021 17:45:30 - INFO - /usr/local/lib/python3.7/dist-packages/datasets/metric.py -   Removing /root/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-06-11 17:45:41,310] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: /content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2500/global_step1703/mp_rank_00_model_states.pt\n",
            "[2021-06-11 17:46:36,141] [INFO] [engine.py:1938:_copy_recovery_script] creating recovery script /content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2500/zero_to_fp32.py\n",
            "[2021-06-11 17:46:36,162] [INFO] [engine.py:1951:_save_zero_checkpoint] zero checkpoint saved /content/drive/MyDrive/GP/student_xsum_4/output/checkpoint-2500/global_step1703/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2021-06-11 17:53:59,963] [INFO] [timer.py:160:stop] 0/8500, SamplesPerSec=18.479279730714435\n",
            "[2021-06-11 18:01:17,262] [INFO] [timer.py:160:stop] 0/9000, SamplesPerSec=18.479194840341012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2954' max='3188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2954/3188 3:49:40 < 56:27, 0.07 it/s, Epoch 3.71/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>6.131500</td>\n",
              "      <td>5.691840</td>\n",
              "      <td>29.645300</td>\n",
              "      <td>9.929800</td>\n",
              "      <td>23.829100</td>\n",
              "      <td>23.819100</td>\n",
              "      <td>21.812200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-06-11 18:08:34,485] [INFO] [timer.py:160:stop] 0/9500, SamplesPerSec=18.478683226674317\n",
            "[2021-06-11 18:15:51,886] [INFO] [timer.py:160:stop] 0/10000, SamplesPerSec=18.477736050236395\n",
            "[2021-06-11 18:23:09,025] [INFO] [timer.py:160:stop] 0/10500, SamplesPerSec=18.4774809733934\n",
            "[2021-06-11 18:30:26,170] [INFO] [timer.py:160:stop] 0/11000, SamplesPerSec=18.477148935121654\n",
            "[2021-06-11 18:37:43,127] [INFO] [timer.py:160:stop] 0/11500, SamplesPerSec=18.477105803654272\n",
            "[2021-06-11 18:45:00,211] [INFO] [timer.py:160:stop] 0/12000, SamplesPerSec=18.476852695298984\n",
            "[2021-06-11 18:52:16,710] [INFO] [timer.py:160:stop] 0/12500, SamplesPerSec=18.477619840712027\n",
            "[2021-06-11 18:55:56,794] [INFO] [logging.py:60:log_dist] [Rank 0] step=2000, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n",
            "[2021-06-11 18:59:33,220] [INFO] [timer.py:160:stop] 0/13000, SamplesPerSec=18.478262398721707\n",
            "[2021-06-11 19:06:50,026] [INFO] [timer.py:160:stop] 0/13500, SamplesPerSec=18.47837813641359\n",
            "[2021-06-11 19:14:07,282] [INFO] [timer.py:160:stop] 0/14000, SamplesPerSec=18.477766325784312\n",
            "[2021-06-11 19:21:24,243] [INFO] [timer.py:160:stop] 0/14500, SamplesPerSec=18.47774501854715\n",
            "[2021-06-11 19:28:41,215] [INFO] [timer.py:160:stop] 0/15000, SamplesPerSec=18.477764962180228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjBDY9sdt0GS"
      },
      "source": [
        "#trainer.save_model(PATH + '/output/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:44.474421Z",
          "iopub.status.idle": "2021-06-04T17:06:44.474943Z"
        },
        "trusted": true,
        "id": "JhFhYGxTsggL"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEoxMuJEEjKu",
        "execution": {
          "iopub.status.busy": "2021-06-04T17:06:44.475953Z",
          "iopub.status.idle": "2021-06-04T17:06:44.476519Z"
        },
        "trusted": true
      },
      "source": [
        "trainer.predict(tokens[\"test\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-HoPm5JPZOJ"
      },
      "source": [
        "import wandb \n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5grL4F3b4ZLu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}