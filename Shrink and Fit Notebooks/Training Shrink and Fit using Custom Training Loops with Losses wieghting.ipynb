{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "pegasusstudenttrainingpipeline-v0-1-beta-tb-ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJHlyrzmZusR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec56a387-cca7-485d-fe93-3da166d00708"
      },
      "source": [
        "!pip install torch transformers datasets sentencepiece\n",
        "!pip install tensorboard scikit-learn psutil sacrebleu rouge-score tensorflow_datasets pytorch-lightning matplotlib git-python faiss-cpu streamlit elasticsearch nltk pandas datasets fire pytest conllu sentencepiece protobuf\n",
        "!pip install jax jaxlib\n",
        "!#pip install torch-lr-finder\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.6.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.5.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (0.82.0)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.7/dist-packages (7.12.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.6.2)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (4.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (3.12.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.32.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (56.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.30.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.0.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.30.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.41.1)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.1.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (21.2.0)\n",
            "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (5.4.1)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.8.1+cu101)\n",
            "Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2021.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (20.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from git-python) (3.1.17)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.18.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.6.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.7.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.7.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning) (3.7.4.post0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->git-python) (4.0.7)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.5.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning) (1.6.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning) (5.1.0)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python) (4.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.0.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.9.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.66+cuda110)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.30)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.17)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "australian-jaguar"
      },
      "source": [
        "# imports \n",
        "import logging\n",
        "from transformers import PegasusTokenizerFast, PegasusForConditionalGeneration,PegasusConfig,AutoTokenizer,AutoModelForSeq2SeqLM\n",
        "import datasets\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from typing import Callable, Dict, Iterable, List, Tuple, Union\n",
        "from transformers import EvalPrediction, PreTrainedTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from transformers import AdamW\n",
        "import wandb\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "editorial-ballot"
      },
      "source": [
        "# Logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atomic-builder"
      },
      "source": [
        "# Teacher model and Tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/pegasus-gigaword')\n",
        "\n",
        "teacher = AutoModelForSeq2SeqLM.from_pretrained('google/pegasus-gigaword')\n",
        "#copy_teacher = AutoModelForSeq2SeqLM.from_pretrained('google/pegasus-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retained-resort",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c31254e-157e-4a77-fd44-6127172673e5"
      },
      "source": [
        "#Student configuration\n",
        "\n",
        "import warnings\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional, Tuple, List, Union\n",
        "from transformers import PegasusModel, PegasusConfig, PegasusForConditionalGeneration\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PreTrainedModel\n",
        "from transformers import SummarizationPipeline\n",
        "\n",
        "students_config_book = {\n",
        "    '2': PegasusConfig(encoder_layers=2, decoder_layers=2),\n",
        "    '4': PegasusConfig(encoder_layers=4, decoder_layers=4),\n",
        "    '6': PegasusConfig(encoder_layers=6, decoder_layers=6),\n",
        "    '8': PegasusConfig(encoder_layers=8, decoder_layers=8),\n",
        "    '10': PegasusConfig(encoder_layers=10, decoder_layers=10),\n",
        "    '12': PegasusConfig(encoder_layers=12, decoder_layers=12),\n",
        "    '16': PegasusConfig(encoder_layers=16, decoder_layers=16)\n",
        "}\n",
        "\n",
        "\n",
        "LAYERS_TO_COPY = {   \n",
        "    4:{\n",
        "        1: [0],\n",
        "        2: [0, 3],\n",
        "        3: [0, 1, 3],\n",
        "        4: [0, 1, 2, 3],\n",
        "    },\n",
        "    8:{\n",
        "        1: [0],\n",
        "        2: [0, 7],\n",
        "        3: [0, 4, 7],\n",
        "        4: [0, 3, 6, 7],\n",
        "        6: [0, 2, 3, 5, 6, 7],\n",
        "        8: list(range(8)),  \n",
        "    },    \n",
        "    12: {\n",
        "        1: [0],\n",
        "        2: [0, 11],\n",
        "        3: [0, 6, 11],\n",
        "        4: [0, 4, 9, 11],\n",
        "        6: [0, 2, 5, 8, 10, 11],\n",
        "        8: [0, 1, 3, 5, 7, 9, 10, 11],\n",
        "        12: list(range(12)),  \n",
        "    },\n",
        "    16: {  # maps  num layers in student -> which teacher layers to copy\n",
        "        1: [0],\n",
        "        2: [0, 15],\n",
        "        3: [0, 8, 15],\n",
        "        4: [0, 5, 10, 15],\n",
        "        6: [0, 3, 6, 9, 12, 15],\n",
        "        8: [0, 2, 4, 6, 8, 10, 12, 15],\n",
        "        9: [0, 1, 3, 5, 7, 9, 11, 13, 15],\n",
        "        12: [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 13, 15],\n",
        "        16: list(range(16)),\n",
        "    },}\n",
        "LAYERS_TO_SUPERVISE = {\n",
        "    # maps  num layers in student -> which teacher layers to copy.\n",
        "    8: {1: [5], 2: [3, 5], 3: [1, 4, 5], 4: [1, 2, 4, 5]},\n",
        "    12: {1: [11], 2: [5, 11], 3: [3, 7, 11], 4:[1, 3, 7, 11],6: [1, 3, 5, 8, 10, 11], 8:[1,2,3,5,7,8,9,11] },\n",
        "    16: {1: [15], 4: [4, 9, 12, 15], 8: [1, 3, 5, 7, 9, 11, 13, 15], 12:[1,2,3,5,7,8,9,11,12,13,14,15]},\n",
        "}\n",
        "\n",
        "\n",
        "def copy_layers(src_layers: nn.ModuleList, dest_layers: nn.ModuleList, layers_to_copy) -> None:\n",
        "    layers_to_copy = nn.ModuleList([src_layers[i] for i in layers_to_copy])\n",
        "    assert len(dest_layers) == len(\n",
        "        layers_to_copy), f\"{len(dest_layers)} != {len(layers_to_copy)}\"\n",
        "    dest_layers.load_state_dict(layers_to_copy.state_dict())\n",
        "\n",
        "# Copied from transformers.models.bart.modeling_bart.shift_tokens_right\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n",
        "\n",
        "def pick_layers_to_copy(n_student, n_teacher):\n",
        "    try:\n",
        "        val = LAYERS_TO_COPY[n_teacher][n_student]\n",
        "        return val\n",
        "    except KeyError:\n",
        "        if n_student != n_teacher:\n",
        "            warnings.warn(\n",
        "                f\"no hardcoded layers to copy for teacher {n_teacher} -> student {n_student}, defaulting to first {n_student}\"\n",
        "            )\n",
        "        return list(range(n_student))\n",
        "\n",
        "def get_layers_to_supervise(n_student, n_teacher) -> List[int]:\n",
        "    \"\"\"Used or the --supervise_forward kwarg\"\"\"\n",
        "    if n_student > n_teacher:\n",
        "        raise ValueError(f\"Cannot perform intermediate supervision for student {n_student} > teacher {n_teacher}\")\n",
        "    elif n_teacher == n_student:\n",
        "        return list(range(n_teacher))\n",
        "    elif n_student == 1:\n",
        "        return [n_teacher - 1]\n",
        "    else:\n",
        "        return LAYERS_TO_SUPERVISE[n_teacher][n_student]\n",
        "\n",
        "def create_student_with_configuration(teacher,\n",
        "                                      e=None,\n",
        "                                      d=None,\n",
        "                                      copy_first_teacher_layers = False,\n",
        "                                      save_path='./student'):\n",
        "\n",
        "    teacher.eval()\n",
        "    teacher_e, teacher_d = teacher.config.encoder_layers, teacher.config.decoder_layers\n",
        "    init_kwargs = teacher.config.to_diff_dict()\n",
        "    if e is None:\n",
        "        e = teacher_e\n",
        "    if d is None:\n",
        "        d = teacher_d\n",
        "    init_kwargs.update({\"encoder_layers\": e, \"decoder_layers\": d})\n",
        "    student_cfg = teacher.config_class(**init_kwargs)\n",
        "    student = AutoModelForSeq2SeqLM.from_config(student_cfg)\n",
        "    # Start by copying the full teacher state dict this will copy the first N teacher layers to the student.\n",
        "    info = student.load_state_dict(teacher.state_dict(), strict=False)\n",
        "    # every student key should have a teacher keys.\n",
        "    assert info.missing_keys == [], info.missing_keys\n",
        "\n",
        "    if copy_first_teacher_layers:  # Our copying is done. We just log and save\n",
        "        e_layers_to_copy, d_layers_to_copy = list(range(e)), list(range(d))\n",
        "        #student.save_pretrained(save_path)\n",
        "        return student, e_layers_to_copy, d_layers_to_copy\n",
        "\n",
        "    # Decide which layers of the teacher to copy. Not exactly alternating -- we try to keep first and last layer.\n",
        "    e_layers_to_copy: List[int] = pick_layers_to_copy(e, teacher_e)\n",
        "    d_layers_to_copy: List[int] = pick_layers_to_copy(d, teacher_d)\n",
        "\n",
        "    copy_layers(teacher.model.encoder.layers,\n",
        "                student.model.encoder.layers, e_layers_to_copy)\n",
        "    copy_layers(teacher.model.decoder.layers,\n",
        "                student.model.decoder.layers, d_layers_to_copy)\n",
        "\n",
        "    student.config.init_metadata = dict(\n",
        "        teacher_type=teacher.config.model_type,\n",
        "        copied_encoder_layers=e_layers_to_copy,\n",
        "        copied_decoder_layers=d_layers_to_copy,\n",
        "    )\n",
        "    #student.save_pretrained(save_path)\n",
        "    # Save information about copying for easier reproducibility\n",
        "\n",
        "    return student, e_layers_to_copy, d_layers_to_copy\n",
        "#student = create_student_with_configuration(teacher,\n",
        "#                                      e=4,\n",
        "#                                      d=4,\n",
        "#                                      copy_first_teacher_layers = False,\n",
        "#                                      save_path='./student')\n",
        "#import gc\n",
        "#del copy_teacher\n",
        "gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enclosed-saint"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcmVg2uUjSxd"
      },
      "source": [
        "#student"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clean-biography"
      },
      "source": [
        "class PegasusDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels[\"input_ids\"])\n",
        "\n",
        "\n",
        "def prepare_data(model_name, \n",
        "                 train_texts, train_labels, \n",
        "                 val_texts=None, val_labels=None, \n",
        "                 test_texts=None, test_labels=None):\n",
        "  \"\"\"\n",
        "  Prepare input data for model fine-tuning\n",
        "  \"\"\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  prepare_val = False if val_texts is None or val_labels is None else True\n",
        "  prepare_test = False if test_texts is None or test_labels is None else True\n",
        "\n",
        "  def tokenize_data(texts, labels):\n",
        "    encodings = tokenizer(texts, truncation=True, padding='longest')\n",
        "    decodings = tokenizer(labels, truncation=True, padding='longest')\n",
        "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
        "    return dataset_tokenized\n",
        "\n",
        "  train_dataset = tokenize_data(train_texts, train_labels)\n",
        "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
        "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE9xJYz9g8VU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda2ec03-073f-47ea-8d56-e7fb7621bedd"
      },
      "source": [
        "dataset = datasets.load_dataset('gigaword')\n",
        "\n",
        "#source data\n",
        "train_texts, train_labels = dataset['train']['document'][:100000], dataset['train']['summary'][:100000]\n",
        "valid_texts, valid_labels = dataset['validation']['document'][:10000], dataset['validation']['summary'][:10000]\n",
        "test_texts, test_labels = dataset['test']['document'][:1000], dataset['test']['summary'][:1000]\n",
        "train_dataset, valid_dataset, test_dataset = prepare_data('google/pegasus-gigaword', train_texts, train_labels,valid_texts, valid_labels,test_texts, test_labels)\n",
        "\n",
        "del dataset \n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/16/2021 10:45:23 - WARNING - datasets.builder -   Using custom data configuration default\n",
            "05/16/2021 10:45:23 - WARNING - datasets.builder -   Reusing dataset gigaword (/root/.cache/huggingface/datasets/gigaword/default/1.2.0/ea83a8b819190acac5f2dae011fad51dccf269a0604ec5dd24795b64efb424b6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emLntAiTk4lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c48246-47e9-4753-c3ac-3bce0548976a"
      },
      "source": [
        "len(train_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fantastic-chancellor"
      },
      "source": [
        "#train_dataloader = DataLoader(train_dataset,batch_size=128)\n",
        "#test_dataloader = DataLoader(test_dataset,batch_size=128)\n",
        "#validation_dataloader = DataLoader(valid_dataset,batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbjRnD2iitUN"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset,batch_size=24,num_workers = 2)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=24,num_workers = 2)\n",
        "validation_dataloader = DataLoader(valid_dataset,batch_size=24,num_workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU0tXTpE5xFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f8e6f0-9129-46dc-cd08-86061ba919c1"
      },
      "source": [
        "#next(iter(train_dataloader))\n",
        "#next(iter(test_dataloader))\n",
        "len(iter(train_dataloader))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "contrary-bracket"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY8B7Vp33GRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667d3b68-27b9-46ce-f9c8-2a6f3bd54b4c"
      },
      "source": [
        "!pip install nltk\n",
        "import re\n",
        "\n",
        "from filelock import FileLock\n",
        "\n",
        "\n",
        "try:\n",
        "    import nltk\n",
        "\n",
        "    NLTK_AVAILABLE = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    NLTK_AVAILABLE = False\n",
        "\n",
        "if NLTK_AVAILABLE:\n",
        "    with FileLock(\".lock\") as lock:\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "\n",
        "def add_newline_to_end_of_each_sentence(x: str) -> str:\n",
        "    \"\"\"This was added to get rougeLsum scores matching published rougeL scores for BART and PEGASUS.\"\"\"\n",
        "    re.sub(\"<n>\", \"\", x)  # remove pegasus newline char\n",
        "    assert NLTK_AVAILABLE, \"nltk must be installed to separate newlines between sentences. (pip install nltk)\"\n",
        "    return \"\\n\".join(nltk.sent_tokenize(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/16/2021 10:46:15 - INFO - filelock -   Lock 140389981684176 acquired on .lock\n",
            "05/16/2021 10:46:16 - INFO - filelock -   Lock 140389981684176 released on .lock\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "changing-surname"
      },
      "source": [
        "## ROUGE Utils\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "ROUGE_KEYS = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "\n",
        "\n",
        "def extract_rouge_mid_statistics(dct):\n",
        "    new_dict = {}\n",
        "    for k1, v1 in dct.items():\n",
        "        mid = v1.mid\n",
        "        new_dict[k1] = {stat: round(getattr(mid, stat), 4) for stat in [\"precision\", \"recall\", \"fmeasure\"]}\n",
        "    return new_dict\n",
        "\n",
        "\n",
        "def calculate_rouge(\n",
        "    pred_lns: List[str],\n",
        "    tgt_lns: List[str],\n",
        "    use_stemmer=True,\n",
        "    rouge_keys=ROUGE_KEYS,\n",
        "    return_precision_and_recall=False,\n",
        "    bootstrap_aggregation=True,\n",
        "    newline_sep=True,\n",
        ") -> Dict:\n",
        "    \"\"\"Calculate rouge using rouge_scorer package.\n",
        "\n",
        "    Args:\n",
        "        pred_lns: list of summaries generated by model\n",
        "        tgt_lns: list of groundtruth summaries (e.g. contents of val.target)\n",
        "        use_stemmer:  Bool indicating whether Porter stemmer should be used to\n",
        "        strip word suffixes to improve matching.\n",
        "        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum\n",
        "        return_precision_and_recall: (False) whether to also return precision and recall.\n",
        "        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False\n",
        "            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``\n",
        "        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL\n",
        "        on multi sentence summaries (CNN/DM dataset).\n",
        "\n",
        "    Returns:\n",
        "         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys\n",
        "\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)\n",
        "    aggregator = scoring.BootstrapAggregator()\n",
        "    for pred, tgt in zip(tgt_lns, pred_lns):\n",
        "        # rougeLsum expects \"\\n\" separated sentences within a summary\n",
        "        if newline_sep:\n",
        "            pred = add_newline_to_end_of_each_sentence(pred)\n",
        "            tgt = add_newline_to_end_of_each_sentence(tgt)\n",
        "        scores = scorer.score(pred, tgt)\n",
        "        aggregator.add_scores(scores)\n",
        "\n",
        "    if bootstrap_aggregation:\n",
        "        result = aggregator.aggregate()\n",
        "        if return_precision_and_recall:\n",
        "            return extract_rouge_mid_statistics(result)  # here we return dict\n",
        "        else:\n",
        "            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n",
        "\n",
        "    else:\n",
        "        return aggregator._scores  # here we return defaultdict(list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaVgrXslEiJV"
      },
      "source": [
        "def freeze(model):\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figured-slide"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stuck-bones"
      },
      "source": [
        "def cross_entropy_loss(logits, labels,label_smoothing,pad_token_id):\n",
        "    lprobs = F.log_softmax(logits, dim=-1)\n",
        "    student_lm_loss, _ = label_smoothed_nll_loss(\n",
        "                lprobs, labels, label_smoothing, ignore_index=pad_token_id\n",
        "            )\n",
        "    return student_lm_loss\n",
        "\n",
        "def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n",
        "    \"\"\"From fairseq\"\"\"\n",
        "    if target.dim() == lprobs.dim() - 1:\n",
        "        target = target.unsqueeze(-1)\n",
        "    nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "    if ignore_index is not None:\n",
        "        pad_mask = target.eq(ignore_index)\n",
        "        nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "        smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "    else:\n",
        "        nll_loss = nll_loss.squeeze(-1)\n",
        "        smooth_loss = smooth_loss.squeeze(-1)\n",
        "\n",
        "    nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\n",
        "    smooth_loss = smooth_loss.sum()\n",
        "    eps_i = epsilon / lprobs.size(-1)\n",
        "    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n",
        "    return loss, nll_loss\n",
        "\n",
        "def calc_hidden_loss(attention_mask, hidden_states, hidden_states_T, matches, normalize_hidden):\n",
        "    \"\"\"MSE(student_hid, teacher_hid[matches]). Called \"Intermediate supervision\" in paper. Inspired by TinyBERT.\"\"\"\n",
        "    msg = \"expected list or tuple for hidden_states, got tensor of shape: \"\n",
        "    mask = attention_mask.to(hidden_states[0])\n",
        "    valid_count = mask.sum() * hidden_states[0].size(-1)\n",
        "    student_states = torch.stack([hidden_states[i] for i in range(len(matches))])\n",
        "    teacher_states = torch.stack([hidden_states_T[j] for j in matches])\n",
        "    if normalize_hidden:\n",
        "        student_states = F.layer_norm(student_states, student_states.shape[1:])\n",
        "        teacher_states = F.layer_norm(teacher_states, teacher_states.shape[1:])\n",
        "    mse = F.mse_loss(student_states, teacher_states, reduction=\"none\")\n",
        "    masked_mse = (mse * mask.unsqueeze(0).unsqueeze(-1)).sum() / valid_count\n",
        "    return masked_mse\n",
        "\n",
        "\n",
        "def calc_ce_loss(mask, s_logits, t_logits,temperature=2):\n",
        "    \"\"\"Copy pasted from distillbert (transformers/examples/distillation/)\"\"\"\n",
        "    # mask has False at padding_idx\n",
        "    sel_mask = mask[:, :, None].expand_as(s_logits)\n",
        "\n",
        "    #print(sel_mask.shape ,s_logits.shape ,t_logits.shape )\n",
        "    \n",
        "    vocab_size = s_logits.size(-1)\n",
        "    s_logits_slct = torch.masked_select(s_logits, sel_mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
        "    t_logits_slct = torch.masked_select(t_logits, sel_mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
        "    s_logits_slct = s_logits_slct.view(-1, vocab_size)  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
        "    t_logits_slct = t_logits_slct.view(-1, vocab_size)  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
        "    ce_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    loss_ce = (\n",
        "        ce_loss_fct(\n",
        "            F.log_softmax(s_logits_slct / temperature, dim=-1),\n",
        "            F.softmax(t_logits_slct / temperature, dim=-1),\n",
        "        )\n",
        "        * (temperature) ** 2 \n",
        "    )\n",
        "    return loss_ce\n",
        "\n",
        "def shift_tokens_right(input_ids, pad_token_id):\n",
        "    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n",
        "    prev_output_tokens = input_ids.clone()\n",
        "    #print(pad_token_id, input_ids)\n",
        "    x= (input_ids.ne(pad_token_id).sum(dim=1) - 1)\n",
        "    index_of_eos = x.unsqueeze(-1)\n",
        "    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
        "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
        "    return prev_output_tokens\n",
        "\n",
        "def blended_loss(teacher,student,batch,labels,e_layer_ids, d_layer_ids,mean_ce, mean_logits, mean_hidden,pad_token_id): #TODO Calculate the total loss\n",
        "        \n",
        "        alpha_hid = 1/mean_hidden\n",
        "        alpha_ce = 1/mean_ce\n",
        "        alpha_mlm= 1/mean_logits\n",
        "        \n",
        "\n",
        "        student_encoder_layers = student.config.encoder_layers\n",
        "        student_decoder_layers = student.config.decoder_layers\n",
        "        teacher_encoder_layers = teacher.config.encoder_layers\n",
        "        teacher_decoder_layers = teacher.config.decoder_layers\n",
        "\n",
        "        e_matches = get_layers_to_supervise(\n",
        "                    n_student=len(e_layer_ids), n_teacher=teacher_encoder_layers\n",
        "                )\n",
        "        d_matches = get_layers_to_supervise(\n",
        "                    n_student=len(d_layer_ids), n_teacher=teacher_decoder_layers\n",
        "                )\n",
        "        \n",
        "        different_base_models = False\n",
        "        do_calc_hidden_loss = (not different_base_models) and alpha_hid > 0\n",
        "        different_encoder = different_base_models or (student.config.encoder_layers != teacher.config.encoder_layers)\n",
        "        \n",
        "        #input_ids,src_mask = batch['input_ids'], batch['attention_mask']\n",
        "        \n",
        "        decoder_input_ids = shift_tokens_right(labels, pad_token_id)\n",
        "        \n",
        "        student_outputs = student(batch['input_ids'],\n",
        "            attention_mask= batch['attention_mask'],\n",
        "            decoder_input_ids= decoder_input_ids,\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=False,\n",
        "            use_cache=False)\n",
        "        \n",
        "       \n",
        "        \n",
        "        #lm_logits = student_outputs[\"logits\"] \n",
        "        label_smoothing = 0.2\n",
        "        student_loss = cross_entropy_loss(student_outputs[\"logits\"], labels,label_smoothing,pad_token_id)\n",
        "\n",
        "        def zero_tensor():\n",
        "            return torch.tensor(0.0).type_as(student_loss)\n",
        "        \n",
        "        teacher_enc_outputs = student_outputs[\n",
        "            \"encoder_last_hidden_state\"\n",
        "        ]  # use this unless self.different_base_models\n",
        "        hid_loss_enc, hid_loss_dec = zero_tensor(), zero_tensor()\n",
        "        \n",
        "        \n",
        "        \n",
        "        if different_encoder:  # compute encoder hidden state loss\n",
        "            all_teacher_encoder_outputs = teacher.get_encoder()(\n",
        "                batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                output_hidden_states=True,\n",
        "            )\n",
        "            #print(all_teacher_encoder_outputs['hidden_states'].shape)\n",
        "            if different_base_models:\n",
        "                teacher_enc_outputs = all_teacher_encoder_outputs[\"last_hidden_state\"]\n",
        "            elif do_calc_hidden_loss:\n",
        "                hid_loss_enc = calc_hidden_loss(\n",
        "                    batch['attention_mask'],\n",
        "                    student_outputs[\"encoder_hidden_states\"],\n",
        "                    all_teacher_encoder_outputs[\"hidden_states\"],\n",
        "                    e_matches,\n",
        "                    normalize_hidden=True,\n",
        "                )\n",
        "                #wandb.log({'encoder hidden loss': hid_loss_enc})\n",
        "        # decoder_input_ids for teacher [8,1] (zeros)\n",
        "        #decodeIds = torch.cuda.LongTensor([0,0,0,0,0,0,0,0]).reshape(1,8)\n",
        "        \n",
        "      \n",
        "\n",
        "        teacher_outputs = teacher(\n",
        "            batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            encoder_outputs=(teacher_enc_outputs,),\n",
        "            decoder_input_ids= decoder_input_ids,\n",
        "            output_hidden_states=do_calc_hidden_loss,\n",
        "            use_cache=False,  # since we are not passing labels, never let this default to True\n",
        "        )\n",
        "\n",
        "        dec_mask = decoder_input_ids.ne(pad_token_id)\n",
        "        loss_ce = calc_ce_loss(dec_mask, student_outputs[\"logits\"], teacher_outputs[\"logits\"])\n",
        "        if do_calc_hidden_loss:  # Intermediate supervision of decoder hidden states\n",
        "            hid_loss_dec = calc_hidden_loss(\n",
        "                dec_mask,\n",
        "                student_outputs[\"decoder_hidden_states\"],\n",
        "                teacher_outputs[\"decoder_hidden_states\"],\n",
        "                d_matches,\n",
        "                normalize_hidden=True,\n",
        "            )\n",
        "            #wandb.log({'decoder hidden loss': hid_loss_dec})\n",
        "        \n",
        "        \n",
        "        losses = {'ce_KD_loss':loss_ce, 'student loss Logits':student_loss, 'Hidden': hid_loss_dec}\n",
        "        blended_loss = (\n",
        "            alpha_ce * loss_ce\n",
        "            + alpha_mlm * student_loss\n",
        "            + alpha_hid * (hid_loss_enc + hid_loss_dec)\n",
        "        )\n",
        "\n",
        "        del student_outputs\n",
        "        del teacher_outputs\n",
        "        del batch\n",
        "        del labels\n",
        "        del all_teacher_encoder_outputs\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return blended_loss, losses\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skilled-adjustment"
      },
      "source": [
        "# Training Loop "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5-1OXnljq72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1a9154-6c6a-4489-c119-4701358bbc5c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 16 10:46:16 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6SgxCZPgpMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691522cf-09f7-4b27-8db6-76c3bfbf2faa"
      },
      "source": [
        "student, e_layers_list, d_layers_list = create_student_with_configuration(\n",
        "                                      teacher,\n",
        "                                      e=4,\n",
        "                                      d=4,\n",
        "                                      copy_first_teacher_layers = False,\n",
        "                                      save_path='./student')\n",
        "\n",
        "optimizer = AdamW(student.parameters(), lr=5e-5)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "student.to('cuda')\n",
        "teacher.to('cuda')\n",
        "!nvidia-smi\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 16 10:46:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W /  70W |   4050MiB / 15109MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aCS8CqpFKN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037fc685-14dd-452f-c26b-cf9515413fa9"
      },
      "source": [
        "#freeze the teacher model \n",
        "freeze(teacher)\n",
        "!nvidia-smi\n",
        "for param in student.model.shared.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in student.model.encoder.embed_tokens.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in student.model.encoder.embed_positions.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in student.model.decoder.embed_tokens.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in student.model.decoder.embed_positions.parameters():\n",
        "  param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 16 10:46:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W /  70W |   4050MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTO4jDsy4w3g"
      },
      "source": [
        " # Evaluation LOOP\n",
        "with torch.no_grad():\n",
        "      student.eval()\n",
        "      \n",
        "      test_loss_logits=[]\n",
        "      test_loss_hidden=[]\n",
        "      test_loss_ce=[]\n",
        "      for i, test_batch in enumerate(test_dataloader):\n",
        "          y = test_batch['labels'].to('cuda')\n",
        "          x = {\n",
        "                  'input_ids':test_batch['input_ids'].to('cuda'),\n",
        "                  'attention_mask':test_batch['attention_mask'].to('cuda')\n",
        "              } \n",
        "            \n",
        "          prediction = student.generate(**x)\n",
        "        \n",
        "          loss , all_losses = blended_loss(teacher,student,x,y,e_layers_list, d_layers_list,1,1,1,pad_token_id)      \n",
        "          test_loss_logits.append(all_losses['student loss Logits'])\n",
        "          test_loss_hidden.append(all_losses['Hidden'])\n",
        "          test_loss_ce.append(all_losses['ce_KD_loss'])\n",
        "          #wandb.log(all_losses)\n",
        "          writer.add_scalars(\"Losses/initial Evaluation\", all_losses,i)\n",
        "          del x\n",
        "          del y\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "          \n",
        "      mean_ce, mean_logits, mean_hidden = torch.mean(torch.tensor(test_loss_ce)), torch.mean(torch.tensor(test_loss_logits)), torch.mean(torch.tensor(test_loss_hidden))   \n",
        "      writer.flush()  \n",
        "      del test_loss_ce \n",
        "      del test_loss_logits \n",
        "      del test_loss_hidden\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAzW_qtJYb3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45eeb5de-a84c-4a96-da54-a84855bac3c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "industrial-threat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c73cec-ab59-4178-819f-5334f2953833"
      },
      "source": [
        "  # ----------------\n",
        "  # TRAINING LOOP\n",
        "  # ----------------\n",
        "  num_epochs = 5\n",
        "  for epoch in range(num_epochs):\n",
        "    i = 0\n",
        "    # TRAINING LOOP\n",
        "    for train_batch in train_dataloader:\n",
        "      i=i+1\n",
        "      y = train_batch['labels'].to('cuda')\n",
        "      x = {\n",
        "          'input_ids':train_batch['input_ids'].to('cuda'),\n",
        "          'attention_mask':train_batch['attention_mask'].to('cuda')\n",
        "      }\n",
        "      #print(y , x)\n",
        "      decoder_input_ids = shift_tokens_right(y, pad_token_id)\n",
        "\n",
        "      # [ 8 zeros -> feed only\n",
        "      #wandb.watch(student)\n",
        "      \n",
        "      student.train(True)\n",
        "      teacher.eval()\n",
        "\n",
        "      student(x['input_ids'],\n",
        "              attention_mask=x['attention_mask'],\n",
        "              decoder_input_ids=decoder_input_ids,\n",
        "              output_hidden_states=True,\n",
        "              output_attentions=False,\n",
        "              use_cache=False)\n",
        "\n",
        "      loss, all_losses = blended_loss(teacher, student, x, y, e_layers_list, d_layers_list, mean_ce, mean_logits, mean_hidden, pad_token_id)\n",
        "\n",
        "      print(f'epoch|{epoch} iteration {i} train loss: {loss.item()}')\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      #wandb.log({\"loss\": loss})\n",
        "      writer.add_scalar(\"Loss/train\", loss, i)  \n",
        "      #print(torch.cuda.get_device_name(0))\n",
        "      #print('Memory Usage:')\n",
        "      #print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "      #print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
        "      del x\n",
        "      del y\n",
        "      del train_batch\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "    writer.flush()\n",
        "\n",
        "    # VALIDATION LOOP\n",
        "    with torch.no_grad():\n",
        "      val_loss = []\n",
        "      student.eval()\n",
        "      for val_batch in validation_dataloader:\n",
        "          y = val_batch['labels'].to('cuda')\n",
        "          x = {\n",
        "                'input_ids':val_batch['input_ids'].to('cuda'),\n",
        "                'attention_mask':val_batch['attention_mask'].to('cuda')\n",
        "            } \n",
        "          decoder_input_ids = shift_tokens_right(y, pad_token_id)\n",
        "              \n",
        "          logits = student(x['input_ids'],\n",
        "                attention_mask=x['attention_mask'],\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                output_hidden_states=True,\n",
        "                output_attentions=False,\n",
        "                use_cache=False)\n",
        "          \n",
        "          loss , all_losses  = blended_loss(teacher,student,x,y,e_layers_list, d_layers_list,mean_ce, mean_logits, mean_hidden,pad_token_id) \n",
        "          val_loss.append(loss.item())\n",
        "          del x\n",
        "          del y\n",
        "          del val_batch\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      val_losses = torch.mean(torch.tensor(val_loss))\n",
        "      print('val_loss: ', val_losses.item())\n",
        "\n",
        "\n",
        "    # Evaluation LOOP\n",
        "    with torch.no_grad():\n",
        "      student.eval()\n",
        "      all_labels = []\n",
        "      all_preds = []\n",
        "      test_loss=[]\n",
        "      for i, test_batch in enumerate(test_dataloader):\n",
        "          y = test_batch['labels'].to('cuda')\n",
        "          x = {\n",
        "                  'input_ids':test_batch['input_ids'].to('cuda'),\n",
        "                  'attention_mask':test_batch['attention_mask'].to('cuda')\n",
        "              } \n",
        "            \n",
        "          prediction = student.generate(**x)\n",
        "        \n",
        "          all_labels.append(y)\n",
        "          all_preds.append(prediction)\n",
        "          loss , all_losses = blended_loss(teacher,student,x,y,e_layers_list, d_layers_list,mean_ce, mean_logits, mean_hidden,pad_token_id)       \n",
        "          test_loss.append(loss.item())\n",
        "          #wandb.log(all_losses)\n",
        "          writer.add_scalars(\"Losses/Evaluation\", all_losses, i)\n",
        "\n",
        "      test_losses = torch.mean(torch.tensor(test_loss))\n",
        "      print('test_loss: ', test_losses.item())\n",
        "      preds = [tokenizer.decode(pred[0]) for pred in all_preds]\n",
        "      lbls = [tokenizer.decode(lbl[0]) for lbl in all_labels]\n",
        "      rouge_score = calculate_rouge(pred_lns=preds,tgt_lns=lbls)\n",
        "      writer.add_scalars(\"RougeScores/Evaluation\", rouge_score, epoch)\n",
        "\n",
        "      #wandb.log(rouge_score)\n",
        "      writer.flush()\n",
        "      print(rouge_score)\n",
        "\n",
        "    PATH = f'/content/drive/MyDrive/GP/student_checkpoint_epoch_{epoch}.pt'\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': student.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': all_losses,\n",
        "            }, PATH)\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch|0 iteration 1 train loss: 4.289752960205078\n",
            "epoch|0 iteration 2 train loss: 4.206548690795898\n",
            "epoch|0 iteration 3 train loss: 4.083798885345459\n",
            "epoch|0 iteration 4 train loss: 4.094593048095703\n",
            "epoch|0 iteration 5 train loss: 3.8483574390411377\n",
            "epoch|0 iteration 6 train loss: 3.877967357635498\n",
            "epoch|0 iteration 7 train loss: 3.757674217224121\n",
            "epoch|0 iteration 8 train loss: 3.7347192764282227\n",
            "epoch|0 iteration 9 train loss: 3.702779531478882\n",
            "epoch|0 iteration 10 train loss: 3.6680104732513428\n",
            "epoch|0 iteration 11 train loss: 3.6243414878845215\n",
            "epoch|0 iteration 12 train loss: 3.5978105068206787\n",
            "epoch|0 iteration 13 train loss: 3.585702419281006\n",
            "epoch|0 iteration 14 train loss: 3.5396499633789062\n",
            "epoch|0 iteration 15 train loss: 3.599029064178467\n",
            "epoch|0 iteration 16 train loss: 3.6442253589630127\n",
            "epoch|0 iteration 17 train loss: 3.5897066593170166\n",
            "epoch|0 iteration 18 train loss: 3.4608511924743652\n",
            "epoch|0 iteration 19 train loss: 3.538313865661621\n",
            "epoch|0 iteration 20 train loss: 3.527496576309204\n",
            "epoch|0 iteration 21 train loss: 3.4228391647338867\n",
            "epoch|0 iteration 22 train loss: 3.4220032691955566\n",
            "epoch|0 iteration 23 train loss: 3.4155852794647217\n",
            "epoch|0 iteration 24 train loss: 3.479140520095825\n",
            "epoch|0 iteration 25 train loss: 3.3553295135498047\n",
            "epoch|0 iteration 26 train loss: 3.3875250816345215\n",
            "epoch|0 iteration 27 train loss: 3.345366954803467\n",
            "epoch|0 iteration 28 train loss: 3.399087429046631\n",
            "epoch|0 iteration 29 train loss: 3.4076173305511475\n",
            "epoch|0 iteration 30 train loss: 3.592823028564453\n",
            "epoch|0 iteration 31 train loss: 3.4546046257019043\n",
            "epoch|0 iteration 32 train loss: 3.4573962688446045\n",
            "epoch|0 iteration 33 train loss: 3.4534361362457275\n",
            "epoch|0 iteration 34 train loss: 3.4349851608276367\n",
            "epoch|0 iteration 35 train loss: 3.3206427097320557\n",
            "epoch|0 iteration 36 train loss: 3.2619919776916504\n",
            "epoch|0 iteration 37 train loss: 3.2511048316955566\n",
            "epoch|0 iteration 38 train loss: 3.3224358558654785\n",
            "epoch|0 iteration 39 train loss: 3.3099145889282227\n",
            "epoch|0 iteration 40 train loss: 3.3557534217834473\n",
            "epoch|0 iteration 41 train loss: 3.2920541763305664\n",
            "epoch|0 iteration 42 train loss: 3.3089842796325684\n",
            "epoch|0 iteration 43 train loss: 3.244168281555176\n",
            "epoch|0 iteration 44 train loss: 3.362267017364502\n",
            "epoch|0 iteration 45 train loss: 3.245494842529297\n",
            "epoch|0 iteration 46 train loss: 3.2564287185668945\n",
            "epoch|0 iteration 47 train loss: 3.275012969970703\n",
            "epoch|0 iteration 48 train loss: 3.317878484725952\n",
            "epoch|0 iteration 49 train loss: 3.2607486248016357\n",
            "epoch|0 iteration 50 train loss: 3.3546717166900635\n",
            "epoch|0 iteration 51 train loss: 3.4061226844787598\n",
            "epoch|0 iteration 52 train loss: 3.2437691688537598\n",
            "epoch|0 iteration 53 train loss: 3.2299904823303223\n",
            "epoch|0 iteration 54 train loss: 3.2601747512817383\n",
            "epoch|0 iteration 55 train loss: 3.1909737586975098\n",
            "epoch|0 iteration 56 train loss: 3.289419412612915\n",
            "epoch|0 iteration 57 train loss: 3.3369224071502686\n",
            "epoch|0 iteration 58 train loss: 3.222933769226074\n",
            "epoch|0 iteration 59 train loss: 3.214663028717041\n",
            "epoch|0 iteration 60 train loss: 3.192483425140381\n",
            "epoch|0 iteration 61 train loss: 3.2603068351745605\n",
            "epoch|0 iteration 62 train loss: 3.242365598678589\n",
            "epoch|0 iteration 63 train loss: 3.2088499069213867\n",
            "epoch|0 iteration 64 train loss: 3.2270898818969727\n",
            "epoch|0 iteration 65 train loss: 3.1988096237182617\n",
            "epoch|0 iteration 66 train loss: 3.2710256576538086\n",
            "epoch|0 iteration 67 train loss: 3.221947193145752\n",
            "epoch|0 iteration 68 train loss: 3.2247121334075928\n",
            "epoch|0 iteration 69 train loss: 3.2823069095611572\n",
            "epoch|0 iteration 70 train loss: 3.4215409755706787\n",
            "epoch|0 iteration 71 train loss: 3.2853994369506836\n",
            "epoch|0 iteration 72 train loss: 3.2482376098632812\n",
            "epoch|0 iteration 73 train loss: 3.2864773273468018\n",
            "epoch|0 iteration 74 train loss: 3.297490119934082\n",
            "epoch|0 iteration 75 train loss: 3.2456767559051514\n",
            "epoch|0 iteration 76 train loss: 3.267092704772949\n",
            "epoch|0 iteration 77 train loss: 3.281008005142212\n",
            "epoch|0 iteration 78 train loss: 3.2008328437805176\n",
            "epoch|0 iteration 79 train loss: 3.203094482421875\n",
            "epoch|0 iteration 80 train loss: 3.247087001800537\n",
            "epoch|0 iteration 81 train loss: 3.2061657905578613\n",
            "epoch|0 iteration 82 train loss: 3.22420334815979\n",
            "epoch|0 iteration 83 train loss: 3.3797733783721924\n",
            "epoch|0 iteration 84 train loss: 3.230356216430664\n",
            "epoch|0 iteration 85 train loss: 3.2569828033447266\n",
            "epoch|0 iteration 86 train loss: 3.2884864807128906\n",
            "epoch|0 iteration 87 train loss: 3.233393430709839\n",
            "epoch|0 iteration 88 train loss: 3.2759292125701904\n",
            "epoch|0 iteration 89 train loss: 3.2583980560302734\n",
            "epoch|0 iteration 90 train loss: 3.2288737297058105\n",
            "epoch|0 iteration 91 train loss: 3.306657075881958\n",
            "epoch|0 iteration 92 train loss: 3.179312229156494\n",
            "epoch|0 iteration 93 train loss: 3.2227001190185547\n",
            "epoch|0 iteration 94 train loss: 3.232208251953125\n",
            "epoch|0 iteration 95 train loss: 3.1822447776794434\n",
            "epoch|0 iteration 96 train loss: 3.295820713043213\n",
            "epoch|0 iteration 97 train loss: 3.2442972660064697\n",
            "epoch|0 iteration 98 train loss: 3.281717300415039\n",
            "epoch|0 iteration 99 train loss: 3.2090415954589844\n",
            "epoch|0 iteration 100 train loss: 3.282454490661621\n",
            "epoch|0 iteration 101 train loss: 3.2631683349609375\n",
            "epoch|0 iteration 102 train loss: 3.1938815116882324\n",
            "epoch|0 iteration 103 train loss: 3.281222343444824\n",
            "epoch|0 iteration 104 train loss: 3.243298053741455\n",
            "epoch|0 iteration 105 train loss: 3.218184471130371\n",
            "epoch|0 iteration 106 train loss: 3.289735794067383\n",
            "epoch|0 iteration 107 train loss: 3.250941753387451\n",
            "epoch|0 iteration 108 train loss: 3.1758532524108887\n",
            "epoch|0 iteration 109 train loss: 3.403050661087036\n",
            "epoch|0 iteration 110 train loss: 3.2244973182678223\n",
            "epoch|0 iteration 111 train loss: 3.281020164489746\n",
            "epoch|0 iteration 112 train loss: 3.2886972427368164\n",
            "epoch|0 iteration 113 train loss: 3.2274653911590576\n",
            "epoch|0 iteration 114 train loss: 3.2002270221710205\n",
            "epoch|0 iteration 115 train loss: 3.3088486194610596\n",
            "epoch|0 iteration 116 train loss: 3.2525272369384766\n",
            "epoch|0 iteration 117 train loss: 3.2624452114105225\n",
            "epoch|0 iteration 118 train loss: 3.2218923568725586\n",
            "epoch|0 iteration 119 train loss: 3.261106491088867\n",
            "epoch|0 iteration 120 train loss: 3.270718812942505\n",
            "epoch|0 iteration 121 train loss: 3.2513909339904785\n",
            "epoch|0 iteration 122 train loss: 3.2762632369995117\n",
            "epoch|0 iteration 123 train loss: 3.273009777069092\n",
            "epoch|0 iteration 124 train loss: 3.237719774246216\n",
            "epoch|0 iteration 125 train loss: 3.171719551086426\n",
            "epoch|0 iteration 126 train loss: 3.207949638366699\n",
            "epoch|0 iteration 127 train loss: 3.2233452796936035\n",
            "epoch|0 iteration 128 train loss: 3.257626533508301\n",
            "epoch|0 iteration 129 train loss: 3.211193084716797\n",
            "epoch|0 iteration 130 train loss: 3.182774543762207\n",
            "epoch|0 iteration 131 train loss: 3.247697353363037\n",
            "epoch|0 iteration 132 train loss: 3.1824371814727783\n",
            "epoch|0 iteration 133 train loss: 3.2003512382507324\n",
            "epoch|0 iteration 134 train loss: 3.1872141361236572\n",
            "epoch|0 iteration 135 train loss: 3.2223477363586426\n",
            "epoch|0 iteration 136 train loss: 3.213481903076172\n",
            "epoch|0 iteration 137 train loss: 3.217560052871704\n",
            "epoch|0 iteration 138 train loss: 3.255263328552246\n",
            "epoch|0 iteration 139 train loss: 3.2224059104919434\n",
            "epoch|0 iteration 140 train loss: 3.2647013664245605\n",
            "epoch|0 iteration 141 train loss: 3.250418186187744\n",
            "epoch|0 iteration 142 train loss: 3.251908779144287\n",
            "epoch|0 iteration 143 train loss: 3.259079694747925\n",
            "epoch|0 iteration 144 train loss: 3.249021530151367\n",
            "epoch|0 iteration 145 train loss: 3.2120046615600586\n",
            "epoch|0 iteration 146 train loss: 3.3247838020324707\n",
            "epoch|0 iteration 147 train loss: 3.2483761310577393\n",
            "epoch|0 iteration 148 train loss: 3.313202381134033\n",
            "epoch|0 iteration 149 train loss: 3.235621452331543\n",
            "epoch|0 iteration 150 train loss: 3.2334675788879395\n",
            "epoch|0 iteration 151 train loss: 3.209895133972168\n",
            "epoch|0 iteration 152 train loss: 3.2189741134643555\n",
            "epoch|0 iteration 153 train loss: 3.2206976413726807\n",
            "epoch|0 iteration 154 train loss: 3.2869999408721924\n",
            "epoch|0 iteration 155 train loss: 3.3599894046783447\n",
            "epoch|0 iteration 156 train loss: 3.244018077850342\n",
            "epoch|0 iteration 157 train loss: 3.361187219619751\n",
            "epoch|0 iteration 158 train loss: 3.251852035522461\n",
            "epoch|0 iteration 159 train loss: 3.2117018699645996\n",
            "epoch|0 iteration 160 train loss: 3.3257293701171875\n",
            "epoch|0 iteration 161 train loss: 3.2340922355651855\n",
            "epoch|0 iteration 162 train loss: 3.235236406326294\n",
            "epoch|0 iteration 163 train loss: 3.2711739540100098\n",
            "epoch|0 iteration 164 train loss: 3.1975317001342773\n",
            "epoch|0 iteration 165 train loss: 3.2492027282714844\n",
            "epoch|0 iteration 166 train loss: 3.2191314697265625\n",
            "epoch|0 iteration 167 train loss: 3.4849817752838135\n",
            "epoch|0 iteration 168 train loss: 3.2708263397216797\n",
            "epoch|0 iteration 169 train loss: 3.2516348361968994\n",
            "epoch|0 iteration 170 train loss: 3.227031707763672\n",
            "epoch|0 iteration 171 train loss: 3.256446123123169\n",
            "epoch|0 iteration 172 train loss: 3.2623372077941895\n",
            "epoch|0 iteration 173 train loss: 3.2263991832733154\n",
            "epoch|0 iteration 174 train loss: 3.200857162475586\n",
            "epoch|0 iteration 175 train loss: 3.25998592376709\n",
            "epoch|0 iteration 176 train loss: 3.2659149169921875\n",
            "epoch|0 iteration 177 train loss: 3.273190498352051\n",
            "epoch|0 iteration 178 train loss: 3.25026798248291\n",
            "epoch|0 iteration 179 train loss: 3.2994561195373535\n",
            "epoch|0 iteration 180 train loss: 3.2509546279907227\n",
            "epoch|0 iteration 181 train loss: 3.194951057434082\n",
            "epoch|0 iteration 182 train loss: 3.2722935676574707\n",
            "epoch|0 iteration 183 train loss: 3.30971097946167\n",
            "epoch|0 iteration 184 train loss: 3.3210601806640625\n",
            "epoch|0 iteration 185 train loss: 3.2407472133636475\n",
            "epoch|0 iteration 186 train loss: 3.2883706092834473\n",
            "epoch|0 iteration 187 train loss: 3.209873676300049\n",
            "epoch|0 iteration 188 train loss: 3.2292985916137695\n",
            "epoch|0 iteration 189 train loss: 3.1847307682037354\n",
            "epoch|0 iteration 190 train loss: 3.213639497756958\n",
            "epoch|0 iteration 191 train loss: 3.2750725746154785\n",
            "epoch|0 iteration 192 train loss: 3.2203001976013184\n",
            "epoch|0 iteration 193 train loss: 3.212944746017456\n",
            "epoch|0 iteration 194 train loss: 3.2088279724121094\n",
            "epoch|0 iteration 195 train loss: 3.243410348892212\n",
            "epoch|0 iteration 196 train loss: 3.2259140014648438\n",
            "epoch|0 iteration 197 train loss: 3.2259364128112793\n",
            "epoch|0 iteration 198 train loss: 3.3276424407958984\n",
            "epoch|0 iteration 199 train loss: 3.239305257797241\n",
            "epoch|0 iteration 200 train loss: 3.2656798362731934\n",
            "epoch|0 iteration 201 train loss: 3.297804594039917\n",
            "epoch|0 iteration 202 train loss: 3.3070340156555176\n",
            "epoch|0 iteration 203 train loss: 3.164700746536255\n",
            "epoch|0 iteration 204 train loss: 3.1574032306671143\n",
            "epoch|0 iteration 205 train loss: 3.2600691318511963\n",
            "epoch|0 iteration 206 train loss: 3.188201427459717\n",
            "epoch|0 iteration 207 train loss: 3.234316349029541\n",
            "epoch|0 iteration 208 train loss: 3.212219715118408\n",
            "epoch|0 iteration 209 train loss: 3.194915294647217\n",
            "epoch|0 iteration 210 train loss: 3.260530948638916\n",
            "epoch|0 iteration 211 train loss: 3.2653250694274902\n",
            "epoch|0 iteration 212 train loss: 3.2075681686401367\n",
            "epoch|0 iteration 213 train loss: 3.124060869216919\n",
            "epoch|0 iteration 214 train loss: 3.3089423179626465\n",
            "epoch|0 iteration 215 train loss: 3.1815052032470703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfu3ptUMo735",
        "outputId": "8684f759-a834-439c-9316-d0a63cf6bfe9"
      },
      "source": [
        "!pip install tensorboard --upgrade\n",
        "!tensorboard dev upload --logdir runs --name \"pegasus distillation\" --description \"Simple pipeline for distilling pegasus\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboard in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
            "2021-05-16 10:24:01.793778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Data for the \"text\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
            "Upload started and will continue reading any new data as it's added to the logdir.\n",
            "\n",
            "To stop uploading, press Ctrl-C.\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/fdL5EJYRRKqHaOwdocZUGA/\n",
            "\n",
            "\u001b[1m[2021-05-16T10:24:03]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2021-05-16T10:24:03]\u001b[0m Total uploaded: 142 scalars, 0 tensors, 0 binary objects\n",
            "Ctrl-C\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/main.py\", line 46, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/program.py\", line 276, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 654, in run\n",
            "    return _run(flags, self._experiment_url_callback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 124, in _run\n",
            "    intent.execute(server_info, channel)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 463, in execute\n",
            "    end_message += \"Interrupted.\"\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dox2hDXmtwvb"
      },
      "source": [
        "def pretty_size(size):\n",
        "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
        "\tassert(isinstance(size, torch.Size))\n",
        "\treturn \"  \".join(map(str, size))\n",
        "\n",
        "def dump_tensors(gpu_only=True):\n",
        "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
        "\timport gc\n",
        "\ttotal_size = 0\n",
        "\tfor obj in gc.get_objects():\n",
        "\t\ttry:\n",
        "\t\t\tif torch.is_tensor(obj):\n",
        "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
        "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
        "\t\t\t\t\ttotal_size += obj.numel()\n",
        "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
        "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
        "\t\t\t\t\tprint(\"%s  %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
        "\t\t\t\t\ttotal_size += obj.data.numel()\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tpass        \n",
        "\tprint(\"Total size:\", total_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1AcXtT8GbES"
      },
      "source": [
        "dump_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi-2xfHxGcz3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZknSKlyeI_S"
      },
      "source": [
        "rouge_score"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}