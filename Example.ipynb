{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:572: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  (np.object, string),\n/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:573: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  (np.bool, bool),\n/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/tensorboard/util/tensor_util.py:113: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  np.object: SlowAppendObjectArrayToTensorProto,\n/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/tensorboard/util/tensor_util.py:114: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  np.bool: SlowAppendBoolArrayToTensorProto,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from catalyst import dl\n",
    "from src.runners import DistilMLMRunner\n",
    "# from src.models import DistilpegasusStudentModel, PegasusForMLM #BertForMLM  DistilbertStudentModel\n",
    "from catalyst.core import MetricAggregationCallback\n",
    "from torch.utils.data import DataLoader\n",
    "from src.callbacks import (\n",
    "    CosineLossCallback,\n",
    "    KLDivLossCallback,\n",
    "    MaskedLanguageModelCallback,\n",
    "    MSELossCallback,\n",
    "    PerplexityMetricCallbackDistillation,\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n",
      "Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /home/ibrahim/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0a01b1abede4f646130574f203de57a293ded8a7a11e3406a539453afdfeb2c0...\n",
      "Downloading: 159MB [00:34, 4.63MB/s]\n",
      "Downloading: 376MB [01:21, 4.60MB/s]\n",
      "Downloading: 2.11MB [00:02, 1.04MB/s]\n",
      "Downloading: 46.4MB [01:21, 572kB/s]\n",
      "Downloading: 2.43MB [00:02, 1.03MB/s]\n",
      "Dataset cnn_dailymail downloaded and prepared to /home/ibrahim/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0a01b1abede4f646130574f203de57a293ded8a7a11e3406a539453afdfeb2c0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "PATH_TO_YOUR_DATASET = \"./data\"\n",
    "dataset_1 = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "# dataset = load_dataset('csv', data_files = {'train':[\"./modified_train.csv\"], 'valid':[\"./modified_valid.csv\"]})\n",
    "# dataset = load_dataset('csv', data_files = {'train':[f\"{PATH_TO_YOUR_DATASET}/train.csv\"], 'valid':[f\"{PATH_TO_YOUR_DATASET}/valid.csv\"]})\n",
    "# dataset_valid = load_dataset('csv', data_files = f\"{PATH_TO_YOUR_DATASET}/valid.csv\")\n",
    "# dataset = load_dataset(\"cnn_dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.tokenization_utils.PreTrainedTokenizer'; 'transformers.tokenization_utils' is not a package",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-91cd14d10845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedTokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_for_tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprepare_for_tokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.tokenization_utils.PreTrainedTokenizer'; 'transformers.tokenization_utils' is not a package"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_utils.PreTrainedTokenizer import prepare_for_tokenization\n",
    "prepare_for_tokenization(dataset_1['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "emmed in by the state legislature and bureaucracy. Palin is impulsive. Her charisma is such that she does not need to hold an office to command attention or wield influence. She resigned from the Alaska Oil and Gas Conservation Commission all of a sudden in 2004, plunged into a Republican gubernatorial primary in October 2005 and joined McCain\\'s campaign without hesitation. Two of these three dizzying moves ended up in victory, and one did not. Two out of three isn\\'t bad. Why shouldn\\'t Palin think another gamble might pay off? Palin herself may not know her next move. Speculation about her presidential ambitions is premature, though it will be much easier for her to build a national organization now that she has no professional ties to Alaska. Whatever she does will be noticed, that\\'s for sure. Because the attention lavished on Palin\\'s decision is further evidence of her unwitting ability to bring out deep-seated feelings of admiration -- and loathing -- in people. We will be hearing from Palin, and from the Palin-haters, for a long while to come.',\n",
       " '(CNN) -- Holders Bayern Munich equaled the record for successive Champions League wins held by their manager Pep Guardiola\\'s former side Barcelona as they reached the knockout stages Tuesday. Bayern eased past Czech champions Viktoria Pilsen 1-0 for a ninth straight victory and a perfect record in Group D with 12 points from four games. They were joined in the last 16 by Manchester City, who thrashed CSKA Moscow 5-2 to remain in second place in the same group with nine points. Bayern had won the home match against Pilsen 5-0, but made harder work of it on the road and had to wait until Mario Mandzukic\\'s 62nd minute headed winner. He connected with a Philipp Lahm cross, only six minutes after coming on as substitute, helping the Bundesliga giants to maintain their remarkable winning streak in Europe\\'s premier club competition. It was the sixth successive time Bayern have made it to the last 16, while City were making it for the first time after two previous attempts. Having beaten Norwich 7-0 in an English Premier League match at the City of Manchester Stadium Saturday, Manuel Pellegrini\\'s team again ran riot against their Russian opponents. Sergio Aguero provided the initial impetus with an early penalty and then a delightful second before setting up Alvaro Negredo for the third. Negredo went on to complete his hat-trick in the second half, but defensive frailties saw Seydou Doumbia twice pull back goals for CSKA, the second from the penalty spot. CSKA, who were given a partial stadium ban by UEFA for racist chanting aimed at City midfielder Yaya Toure in the reverse fixture in Moscow, which the visitors won 2-1, have only a Europa League place to play for now. The thumping victory was particularly sweet for Toure. \"I think today is a special day for the club to go through to the second round for the first time,\" he told Sky Sports. City\\'s neighbors Manchester United stayed top of Group A after a goalless draw at Real Sociedad, but had Marouaune Fellaini sent off in the second half. Robin van Persie also missed a penalty after coming on as a late substitute for United. Shakhtar Donetsk and Bayer Leverkusen shared a goalless draw in the Ukraine in the other match played in the group. In Group B, Juventus kept their qualification hopes alive as they held Real Madrid to a 2-2 draw in Turin. Juve led through Arturo Vidal\\'s first half penalty before Cristiano Ronaldo and the world\\'s most expensive player Gareth Bale scored fine goals to put Real ahead. Fernando Llorente scored a crucial leveler for Antonio Conte\\'s men to deny Real for now their passage into the last 16. With FC Copenhagen\\'s 1-0 home win over Galatasaray, Juve dropped to last in the group, but trail the Danes and the Turks by just one point with two rounds to play. Galatasaray must also next travel to Real, who are all but mathematically assured of their place in the knockout stages. Paris Saint Germain must also wait to progress from Group C and needed Zlatan Ibrahimovic\\'s equalizer to secure a 1-1 home draw against Anderlecht. Olympiakos beat Benfica 1-0 to improve their chances of going through, moving to within three points of PSG in the standings.',\n",
       " '(CNN) -- After 18 months of terror and grave devastation, Syrian children are plagued with trauma from witnessing the horrors of war firsthand, an international aid group says. Save the Children released a report on Tuesday called \"Untold Atrocities,\" a collection of accounts from Syrian refugee children. \"A massacre took place in my village. Around 25 people were killed -- I witnessed it with my own eyes,\" said Mohamad, 15, who has fled to Jordan with his family. \"They used different ways to kill people -- electric shocks, throwing machinery and cement blocks on people\\'s heads.\" Hassan, 14, described the use of children as human shields, echoing reports from opposition activists that the Syrian regime had done so. He said his cousin and uncle died when a rocket \"caused a massacre.\" \"Almost every child we\\'ve spoken to has seen family members killed,\" Save the Children said. Even those who survive attacks face dire circumstances. \"When we were being bombed, we had nothing. No food, no water, no toys -- nothing. There was no way to buy food -- the markets and shops were bombed out,\" Ala\\'a, 10, said. \"My father went without food for days because there wasn\\'t enough. I remember watching him tie his stomach with rope so he wouldn\\'t feel so hungry.\" Wael, 16, summarized the trauma this way: . \"I have seen children slaughtered. I don\\'t think I\\'ll ever be OK again.\" In other developments: . Diplomatic front: Obama pledges support, Qatar offers a new plan for Syria . U.S. President Barack Obama used his keynote speech at the U.N. General Assembly on Tuesday to pledge American support for those working for a \"common good\" for Syria -- and sanctions against those doing harm. \"In Syria, the future must not belong to a dictator who massacres his people,\" he said. \"If there is a cause that cries out for protest in the world today, it is a regime that tortures children and shoots rockets at apartment buildings. And we must remain engaged to assure that what began with citizens demanding their rights does not end in a cycle of sectarian violence.\" French President Francois Hollande also had strong words on Syria, saying that areas \"liberated\" by opposition forces should be protected by the United Nations. \"There have been almost 30,000 deaths in the last 18 months -- how many more deaths will we wait for before we act? How can we allow the paralysis of the United Nations to continue?\" he asked. Hollande said France would recognize an opposition government once it is formed, and that the current regime had lost its right to represent the country on the international stage. France has been at the forefront of international efforts to bring about a resolution in Syria. In his address to the General Assembly, Qatar Prime Minister Sheikh Hamad bin Jassim Al Thani said the violence in Syria had reached \"an unacceptable phase\" and urged fellow Arab nations to intervene. \"We have used all available means to get Syria out of the cycle of killing, but that was in vain,\" he said. In light of the U.N. Security Council\\'s failure to act effectively, he said, \"It is better for the Arab countries themselves to interfere out of their national, humanitarian, political and military duties and do what it necessary to stop the bloodshed in Syria ... in order to guarantee a peaceful transition of power in Syria.\" His words come a day after he proposed a \"Plan B\" for solving the Syrian crisis, saying a nonviolent solution is still possible despite more than a year of relentless bloodshed. In an interview with CNN\\'s Christiane Amanpour on Monday, Al Thani said the plan would include havens -- which would require a no-fly zone -- and greater humanitarian aid. \"We wish and we believe that we can solve it peacefully,\" Al Thani said. But, he said, Syrian President Bashar al-Assad has only one solution: \"killing his people to win the war.\" \"I believe within weeks, we should have a Plan B. And there is a responsibility among us,\" he said. \"We are talking about saving the people of Syria.\" UK Foreign Secretary William Hague, who spoke to Amanpour on Tuesday, described the situation in the U.N. Security Council with respect to Syria as being at a \"diplomatic impasse.\" \"We are blocked in the United Nations Security Council from the world being able to put its full weight behind a transitional government in Syria, something that it is obvious solution, obviously part of the solution,\" he said. Hague was referring to Russia and China, which have repeatedly blocked draft resolutions that would take stronger action against al-Assad\\'s regime. The secretary is scheduled to meet with Russian leaders this week to discuss the ongoing crisis. On the ground: Blasts strike a Damascus compound . Dual attacks rattled a Syrian intelligence security compound in Damascus, the regime and opposition activists said Tuesday. The compound was also the site of a major explosion in March. Syrian state-run TV said the two improvised explosive devices were \"planted by terrorists\" in a school building and caused seven injuries. Opposition activists said the Syrian military was using the school building as a base. The new school year has not yet started, Syrian state TV said, so it seems unlikely that children would have been at the site. In June, Human Rights Watch described cases of \"sexual torture\" at the compound, reported by male and female detainees -- many of whom were political activists or simply attended protests. At least 148 people were killed across Syria on Tuesday, according to the Local Coordination Committees of Syria, an opposition group. The highest number of deaths, 44, was reported in Damascus and its suburbs, where regime forces and rebel fighters are engaged in fierce clashes and communities are under aerial bombardment, the LCC said. CNN\\'s Salma Abdelaziz, Saad Abedine, Holly Yan, Samuel Burke and Claire Calzonetti contributed to this report.',\n",
       " '(CNN)  -- A journalist who was interviewing a key political protest leader in Bangkok said the sniper bullet that struck the man came so close that it \"felt like it grazed my head.\" Describing a chaotic scene on the streets of the Thai capital Thursday night, Thomas Fuller of the International Herald Tribune described to CNN how Maj. Gen. Khattiya Sawasdipol was shot in the head as he was interviewing the opposition figure. \"I was facing him, he was answering my questions, looking at me and the bullet hit him in the forehead, from what I could tell,\" Fuller told CNN\\'s Michael Holmes. \"It looks like the bullet came over my head and struck him. I don\\'t have any way of confirming this beyond what I remember from the scene, but it felt like it grazed my head.\" Thomas Fuller describes scene in Bangkok . Fuller and other journalists were interviewing the general -- better known as Seh Daeng -- in makeshift barricades that protesters have set up in downtown Bangkok. The United Front for Democracy (UDD) has turned the posh commercial center of Bangkok into a makeshift fortress, as they continue to demand that Prime Minister Abhisit Vejjajiva dissolve the lower house of Parliament and call new elections. The protesters\\' barricades appear as a combination \"of \\'Mad Max\\' and some medieval scene,\" Fuller said. Bamboo pikes and rubber tire barricades have been formed as a makeshift camp by the protesters, Fuller said. iReport: Are you there? Send your images, video . Fuller said he was just inside the barricades when he was interviewing Seh Daeng. The opposition figure was facing out of the barricades and into Bangkok\\'s business district of tall office buildings. \"He was standing in the same location for a while when I was talking to him but he was moving around, he was gesticulating,\" Fuller said. \"He wasn\\'t standing still, he was bobbing his head.\" Seh Daeng did not appear to be armed or have bodyguards but was dressed in camouflage jacket and a floppy hat, Fuller said. The opposition leader was listed in critical condition from the shooting, his guards said. Violence erupted after Thai authorities set a new deadline to seal off the Bangkok intersection where protesters have gathered by the thousands for the past month. Escalating violence in Bangkok . The government said it has been forced to take action after demonstrators disregarded an ultimatum by Abhisit to vacate the intersection by Wednesday. The Red Shirts support former Prime Minister Thaksin Shinawatra, who was ousted in a bloodless military coup in 2006. What are the protests about? Seh Daeng -- or Red Commander -- is a controversial public figure, even within the protest movement, Fuller said. Some Thai opposition leaders see him as an impediment to a peaceful resolution to the political stalemate that has gripped Thai politics, Fuller said. \"He\\'s a renegade in all sense,\" Fuller said. \"He\\'s a renegade from the army, a hardliner within the protest movement. He told me today he thought they (other opposition leaders) were being cowardly, and he wanted to carry on.\" More than two dozen civilians and military personnel have died in police-protester clashes in the ongoing unrest.',\n",
       " \"(CNN) -- Manchester United returned to the top of the Premier League on Sunday after a 0-0 draw against Tottenham Hotspur at White Hart Lane. A close fought affair -- which marked the 600th league appearance of Ryan Giggs -- ended with honors even despite United defender Rafael Da Silva being sent off for two bookable offences in the 74th minute. His second yellow for a trip on Assou-Ekotto was adjudged to be deliberate, but a clearly incensed Rafael thought otherwise as he remonstrated with referee Mike Dean about the decision. Spurs had the better chances to win the match with Rafael Van der Vaart missing a glorious opportunity to claim three points in the 80th minute after a mistake in the United defence gave him a unopposed shot on goal. But the Dutch international could only curl his shot narrowly over the bar. Kenny Dalglish earned the first point of his second spell in charge of Liverpool as the Merseyside derby against Everton ended in a 2-2 draw at Anfield. Raul Meireles' first goal for Liverpool gave the Reds a deserved lead after 30 minutes in a half in which they dominated. But Everton hit back after the break with two goals in six minutes to take the lead. Sylvain Distin headed home after 46 minutes and Jermaine Beckford stunned the home supporters as he drilled home past Jose Reina to hand the Toffees the lead. But when Tim Howard brought down Maxi Rodriguez following a corner in the 66th minute, Dirk Kuyt made no mistake from the penalty spot. Liverpool and Everton both have 26 points, with Everton one place above their rivals thanks to a superior goal difference. In the other two matches played Sunday, the Tyneside derby ended in a 1-1 draw as Sunderland's Asamoah Gyan scored four minutes into injury time to deny Newcastle the three points and a season double over their arch rivals. A neat backheel by Kevin Nolan had given the Magpies the lead seven minutes into the second half. Newcastle looked destined to win the match until the Ghanaian striker put the ball in the net after a shot from Phil Bardsley shot had been saved by Steve Harper. Sunderland stay in sixth place, while Newcastle are ninth. Aston Villa have moved out of the relegation zone after salvaging a 1-1 draw with local rivals Birmingham at St Andrews. Roger Johnson volleyed home early in the second half to give the home side the lead but James Collins squared things up after 73 minutes when he drove a shot home after Gabriel Agbonlahor had flicked the ball onto him. Birmingham remain in 16th place with 23 points on points, one place and one point ahead of Villa.\",\n",
       " '(CNN) -- When your grandmother is one of the most famous cosmetics moguls in history, it might put a little pressure on you to succeed. But for Aerin Lauder, the 44-year-old granddaughter of Estée Lauder, who founded the eponymous make-up company, the legacy has been an inspiration rather than a burden. Lauder worked her way up through the ranks of the billion-dollar family company for 25 years to the position of style and image director. In 2012, she decided to combine her passion for home décor with her knowledge of beauty, to launch her own lifestyle brand, called AERIN. The businesswoman spoke to CNN\\'s Kirstie Lu Stout about her drive, the importance of saying \"no,\" and how beauty transcends borders or race. CNN: Are you living out your dream today with your own brand, AERIN? Aerin Lauder: I\\'m definitely living my dream. As a little girl, I\\'ve always loved beauty and I loved home, so I\\'ve managed to combine the two into a brand. CNN: What is it like to live up to the heritage of Estee Lauder? AL: It is an amazing legacy and I think she is always looking down at me very proud. She taught me the importance of excellence; so has my uncle, my cousin and my aunt, everyone who works at the company as a family member has really re-emphasized the importance of excellence and perfection. CNN: You had a very international upbringing, growing up in Vienna, Austria, where your father was the U.S. ambassador, then moving to Manhattan. How did that shape who you are today? AL: It shaped a tremendous amount of my vision, style and taste. When we moved to Europe when I was a teenager I really did not want to go. I was happy in my school, with my friends, but looking back on it, it was the best experience I\\'ve ever had. We traveled every weekend, I experienced incredible new cultures, museums, cities and it really opened up my eyes. CNN: You worked your way through the ranks of Estee Lauder and managed to extend the reach of the brand to all corners of the world, partially through casting diverse models. In 2003 you hired the Ethiopian model Liya Kebede, then in 2011, models Liu Wen and Joan Smalls from China and Puerto Rico. Why is that an important thing to do in this business? AL: Because beauty is global. It\\'s the idea that every woman can be beautiful, which is a concept Estée has which is still so modern today. It\\'s the idea of beauty from all over the world. CNN: Are you a detail-oriented person? AL: I\\'m very detail-oriented, which is good and bad. Because I will wake up in the middle of the night, thinking about something or seeing a mistake, thinking about it and I immediately send an email -- I\\'m very focused on details. But I think that is really important because it is my name on that product, and I think it should be the best it can possibly be. CNN: What has been the biggest mistake you have made in your career and how did you overcome it? AL: I think it is very important to learn to say \"no.\" I think it is sometimes important for brands or the creative director to learn to say \"this might be on trend but it is not right for us.\" I launched products and campaigns which I thought might bring in new consumers but in reality would make the existing Estée Lauder one maybe discouraged. CNN: In your business schedule you are balancing work and family. How do you find time and get inspired? AL: I think you can get inspiration from anything. It can be a walk on the beach, it can be a moment with your children, and also the Internet has been a wonderful source for inspiration. You can Google it, search it, look at the beaches for the sensibility of there and feels like you are there. I think the Internet is a great way to get inspiration. CNN: Is luxury attainable? AL: Luxury is definitely attainable. I think it could be anything from a beautiful little gold bowl on your desk, to a very glamorous chandelier, and everything in between. Inspire: Building a billion-dollar empire the Tory Burch way . Learn: Iconic fashion designs, from Coco Chanel to DVF .',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "dataset_1['train']['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/datasets/dataset_dict.py:675: ResourceWarning:\n\nunclosed file <_io.TextIOWrapper name='/home/ibrahim/projects_dsp/dstilPegasus/cnn_dataset/dataset_dict.json' encoding='utf-8'>\n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datasets\n",
    "# dataset_1.save_to_disk('cnn_dataset')\n",
    "data = datasets.load_from_disk('cnn_dataset')\n",
    "# np.save(\"cnn_dataset.npy\", dataset_1)\n",
    "# np.save(\"valid_dataset.npy\", valid_dataset)\n",
    "# train_dataset = np.load('cnn_dataset.npy', allow_pickle=True)\n",
    "# valid_dataset = np.load('valid_dataset.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:308: ResourceWarning:\n\nunclosed file <_io.BufferedReader name='/home/ibrahim/.cache/huggingface/transformers/b3949a7257f9b4eaaf4f7785be079d89e4fec7d1c3763a58e6a2743635877d1f.1acf68c74589da6c7fa3548093824dfc450a54637f4356929bbfea7e294a68f8'>\n\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='tuner007/pegasus_paraphrase', vocab_size=96103, model_max_len=60, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']})"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_YOUR_DATASET = \"./data\"\n",
    "train_df = pd.read_csv(f\"{PATH_TO_YOUR_DATASET}/train.csv\")\n",
    "valid_df = pd.read_csv(f\"{PATH_TO_YOUR_DATASET}/valid.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0   Receiving surgery to increase the volume of y...   \n",
       "1   If you are building a new shed, it is importa...   \n",
       "2   Begin by rubbing the scratched wooden item wi...   \n",
       "3   Fraud is defined as an intentional misreprese...   \n",
       "4   Gmail supports instant messaging over AIM or ...   \n",
       "\n",
       "                                            headline  \n",
       "0  \\nIdentify the benefits and drawbacks of hair ...  \n",
       "1  \\nBuild the shed up off of the ground.,\\nPaint...  \n",
       "2  \\nRub the walnut in.,\\nRub the walnut across t...  \n",
       "3  \\nIdentify fraud.,\\nWrite down relevant inform...  \n",
       "4  \\nUse Google Talk if you have a Google email a...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>headline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Receiving surgery to increase the volume of y...</td>\n      <td>\\nIdentify the benefits and drawbacks of hair ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>If you are building a new shed, it is importa...</td>\n      <td>\\nBuild the shed up off of the ground.,\\nPaint...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Begin by rubbing the scratched wooden item wi...</td>\n      <td>\\nRub the walnut in.,\\nRub the walnut across t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Fraud is defined as an intentional misreprese...</td>\n      <td>\\nIdentify fraud.,\\nWrite down relevant inform...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Gmail supports instant messaging over AIM or ...</td>\n      <td>\\nUse Google Talk if you have a Google email a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "\n",
    "# valid_df.shape\n",
    "valid_df = valid_df.drop('Unnamed: 0', axis=1)\n",
    "train_df = train_df.drop('Unnamed: 0', axis=1)\n",
    "train_df = train_df.dropna()\n",
    "valid_df = valid_df.dropna()\n",
    "valid_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv('modified_train.csv', index=False)\n",
    "valid_df.to_csv('modified_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Union\n",
    "\n",
    "# import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "# import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class LanguageModelingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for (masked) language model task.\n",
    "    Can sort sequnces for efficient padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        tokenizer: Union[\n",
    "            str, transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast\n",
    "        ],\n",
    "        max_seq_length: int = None,\n",
    "        sort: bool = True,\n",
    "        lazy: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (Iterable): Iterable object with text\n",
    "            tokenizer (str or tokenizer): pre trained\n",
    "                huggingface tokenizer or model name\n",
    "            max_seq_length (int): max sequence length to tokenize\n",
    "            sort (bool): If True then sort all sequences by length\n",
    "                for efficient padding\n",
    "            lazy (bool): If True then tokenize and encode sequence\n",
    "                in __getitem__ method\n",
    "                else will tokenize in __init__ also\n",
    "                if set to true sorting is unavialible\n",
    "        \"\"\"\n",
    "        if sort and lazy:\n",
    "            raise Exception(\n",
    "                \"lazy is set to True so we can't sort\"\n",
    "                \" sequences by length.\\n\"\n",
    "                \"You should set sort=False and lazy=True\"\n",
    "                \" if you want to encode text in __get_item__ function\"\n",
    "            )\n",
    "        if isinstance(tokenizer, str):\n",
    "            self.tokenizer = tokenizer\n",
    "        elif isinstance(\n",
    "            tokenizer, transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast\n",
    "            # transformers.tokenization_utils.PreTrainedTokenizer\n",
    "        ):\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"tokenizer argument should be a model name\"\n",
    "                + \" or huggingface PreTrainedTokenizer\"\n",
    "            )\n",
    "            # self.tokenizer = tokenizer\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.lazy = lazy\n",
    "\n",
    "        if lazy:\n",
    "            self.texts = texts\n",
    "\n",
    "        if not lazy:\n",
    "            pbar = tqdm(texts, desc=\"tokenizing texts\")\n",
    "            self.encoded = [\n",
    "                self.tokenizer.encode(text, max_length=max_seq_length)\n",
    "                for text in pbar\n",
    "            ]\n",
    "            if sort:\n",
    "                self.encoded.sort(key=len)\n",
    "\n",
    "        self.length = len(texts)\n",
    "\n",
    "        self._getitem_fn = (\n",
    "            self._getitem_lazy if lazy else self._getitem_encoded\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return length of dataloader\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def _getitem_encoded(self, idx) -> torch.Tensor:\n",
    "        return torch.tensor(self.encoded[idx])\n",
    "\n",
    "    def _getitem_lazy(self, idx) -> torch.Tensor:\n",
    "        encoded = self.tokenizer.encode(\n",
    "            self.texts[idx], max_length=self.max_seq_length\n",
    "        )\n",
    "\n",
    "        return torch.tensor(encoded)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return tokenized and encoded sequence\"\"\"\n",
    "        return self._getitem_fn(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:308: ResourceWarning:\n",
      "\n",
      "unclosed file <_io.BufferedReader name='/home/ibrahim/.cache/huggingface/transformers/b3949a7257f9b4eaaf4f7785be079d89e4fec7d1c3763a58e6a2743635877d1f.1acf68c74589da6c7fa3548093824dfc450a54637f4356929bbfea7e294a68f8'>\n",
      "\n",
      "tokenizing texts: 100%|██████████| 4/4 [00:00<00:00, 1699.99it/s]\n",
      "tokenizing texts: 100%|██████████| 4/4 [00:00<00:00, 5491.72it/s]this is type of tokinzer ========  <class 'transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from src.data import LanguageModelingDataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "print('this is type of tokinzer ======== ',type(tokenizer))\n",
    "\n",
    "train_dataset = LanguageModelingDataset(train_df[\"text\"].tolist(), tokenizer)\n",
    "valid_dataset = LanguageModelingDataset(valid_df[\"text\"].tolist(), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.save(\"train_dataset.npy\", train_dataset)\n",
    "# np.save(\"valid_dataset.npy\", valid_dataset)\n",
    "train_dataset = np.load('train_dataset.npy', allow_pickle=True)\n",
    "valid_dataset = np.load('valid_dataset.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers.data.data_collator import default_data_collator\n",
    "collate_fn = default_data_collator\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, collate_fn=collate_fn, batch_size=2\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, collate_fn=collate_fn, batch_size=2\n",
    ")\n",
    "loaders = {\"train\": train_dataloader, \"valid\": valid_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n",
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "StopIteration",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f3cd1bea3510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(train_dataloader.batch_size)\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    b = next(iter(batch.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import transformers\n",
    "\n",
    "\n",
    "class PegasusForMLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BertForMLM\n",
    "\n",
    "    Simplified huggingface model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"tuner007/pegasus_paraphrase\",\n",
    "        output_logits: bool = True,\n",
    "        output_hidden_states: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: huggingface model name\n",
    "            output_logits: same as in huggingface\n",
    "            output_hidden_states: same as in huggingface\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = transformers.AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_logits=output_logits\n",
    "        )\n",
    "        self.bert = transformers.PegasusForCausalLM.from_pretrained(\n",
    "            model_name, config=self.config\n",
    "        )\n",
    "\n",
    "    def forward(self, *model_args, **model_kwargs):\n",
    "        \"\"\"Forward method\"\"\"\n",
    "        return self.bert(*model_args, **model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "class DistilpegasusStudentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DistilbertStudentModel\n",
    "\n",
    "    Distil model class based on huggingface class but with\n",
    "    initialization in it. Model will take vocabulary\n",
    "    layers from specified teacher model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_model_name: str = \"tuner007/pegasus_paraphrase\",\n",
    "        layers: List[int] = None,\n",
    "        extract: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            teacher_model_name: name of the model to distil\n",
    "            layers: layers indexes to initialize\n",
    "            extract: bool flag, if you want to initialize your model with\n",
    "                layers of the teacher model then set this to true\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if layers is None:\n",
    "            layers = [0, 2, 4, 7, 9, 11, 15]\n",
    "        teacher_config = transformers.AutoConfig.from_pretrained(\n",
    "            teacher_model_name, output_hidden_states=True, output_logits=True\n",
    "        )\n",
    "        teacher = transformers.PegasusForCausalLM.from_pretrained(\n",
    "            teacher_model_name, config=teacher_config\n",
    "        )\n",
    "        distil_sd = None\n",
    "        if extract:\n",
    "            distil_sd = self._extract(teacher, layers)\n",
    "        if teacher_model_name == \"tuner007/pegasus_paraphrase\":\n",
    "            f = open('a7ooo.txt', 'a')\n",
    "            f.write(','.join(map(str, distil_sd)))\n",
    "            f.close()\n",
    "            student_config = transformers.AutoConfig.from_pretrained(\n",
    "                \"tuner007/pegasus_paraphrase\",\n",
    "                output_hidden_states=True,\n",
    "                output_logits=True,\n",
    "            )\n",
    "            self.student = transformers.PegasusForCausalLM.from_pretrained(\n",
    "                \"tuner007/pegasus_paraphrase\",\n",
    "                config=student_config,\n",
    "                state_dict=distil_sd,\n",
    "            )\n",
    "        elif teacher_model_name == \"bert-base-cased\":\n",
    "            student_config = transformers.AutoConfig.from_pretrained(\n",
    "                \"distilbert-base-cased\",\n",
    "                output_hidden_states=True,\n",
    "                output_logits=True,\n",
    "            )\n",
    "            self.student = transformers.DistilBertForMaskedLM.from_pretrained(\n",
    "                \"distilbert-base-cased\",\n",
    "                config=student_config,\n",
    "                state_dict=distil_sd,\n",
    "            )\n",
    "        else:\n",
    "            student_config = transformers.AutoConfig.from_pretrained(\n",
    "                \"distilbert-base-multilingual-cased\",\n",
    "                output_hidden_states=True,\n",
    "                output_logits=True,\n",
    "            )\n",
    "            self.student = transformers.DistilBertForMaskedLM.from_pretrained(\n",
    "                \"distilbert-base-multilingual-cased\",\n",
    "                config=student_config,\n",
    "                state_dict=distil_sd,\n",
    "            )\n",
    "\n",
    "    def forward(self, *model_args, **model_kwargs):\n",
    "        \"\"\"Forward nethod\"\"\"\n",
    "        return self.student(*model_args, **model_kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract(\n",
    "        teacher_model,\n",
    "        layers: List[int],\n",
    "        prefix_teacher: str = \"model\",\n",
    "        prefix_student: str = \"pegasus_student\",\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extracts state dict from teacher model\n",
    "\n",
    "        Args:\n",
    "            teacher_model: model to extract\n",
    "            layers: layers indexes to initialize\n",
    "            prefix_teacher: name of the teacher model\n",
    "            prefix_student: name of the student model\n",
    "        \"\"\"\n",
    "        state_dict = teacher_model.state_dict()\n",
    "        # f = open('state_dict.txt', 'a')\n",
    "        # f.write(','.join(map(str, teacher_model.state_dict())))\n",
    "        # f.close()\n",
    "        compressed_sd = {}\n",
    "\n",
    "        # extract embeddings\n",
    "        for w in [\"embed_tokens\", \"embed_positions\"]:\n",
    "            compressed_sd[\n",
    "                f\"{prefix_student}.decoder.{w}.weight\"\n",
    "            ] = state_dict[f\"{prefix_teacher}.decoder.{w}.weight\"]\n",
    "        print('finished_11111111')\n",
    "        for w in [\"weight\", \"bias\"]:\n",
    "            compressed_sd[\n",
    "                f\"{prefix_student}.decoder.layers.0.self_attn_layer_norm.{w}\"\n",
    "            ] = state_dict[f\"{prefix_teacher}.decoder.layers.0.self_attn_layer_norm.{w}\"]\n",
    "        print('finished_222222')\n",
    "        # extract encoder\n",
    "        std_idx = 0\n",
    "        for teacher_idx in layers:\n",
    "            for w in [\"weight\", \"bias\"]:\n",
    "                compressed_sd[\n",
    "                    f\"{prefix_student}.transformer.layer.{std_idx}.attention.q_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"{prefix_teacher}.decoder.layers.{teacher_idx}.encoder_attn.q_proj.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"{prefix_student}.transformer.layer.{std_idx}.attention.k_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"{prefix_teacher}.decoder.layers.{teacher_idx}.encoder_attn.k_proj.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"{prefix_student}.transformer.layer.{std_idx}.attention.v_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"{prefix_teacher}.decoder.layers.{teacher_idx}.encoder_attn.v_proj.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "                compressed_sd[\n",
    "                    f\"{prefix_student}.transformer.layer.{std_idx}.attention.out_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"{prefix_teacher}.decoder.layers.{teacher_idx}.self_attn.out_proj.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"{prefix_student}.transformer.layer.{std_idx}.sa_layer_norm.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"{prefix_teacher}.decoder.layers.{teacher_idx}.self_attn_layer_norm.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "                compressed_sd[\n",
    "                    f\"{prefix_student}.transformer.layer.{std_idx}.ffn.lin1.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"{prefix_teacher}.decoder.layers.{teacher_idx}.fc1.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"{prefix_student}.transformer.layer.{std_idx}.ffn.lin2.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"{prefix_teacher}.decoder.layers.{teacher_idx}.fc2.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"{prefix_student}.transformer.layer.{std_idx}.output_layer_norm.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"{prefix_teacher}.decoder.layers.{teacher_idx}.final_layer_norm.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "            std_idx += 1\n",
    "        print('finished_3333333333')\n",
    "        # extract vocab\n",
    "        compressed_sd[f\"lm_head.weight\"] = state_dict[\n",
    "            f\"lm_head.weight\"\n",
    "        ]\n",
    "        # compressed_sd[f\"vocab_projector.bias\"] = state_dict[\n",
    "        #     f\"cls.predictions.bias\"\n",
    "        # ]\n",
    "\n",
    "        # for w in [\"weight\", \"bias\"]:\n",
    "        #     compressed_sd[f\"vocab_transform.{w}\"] = state_dict[\n",
    "        #         f\"cls.predictions.transform.dense.{w}\"\n",
    "        #     ]\n",
    "        #     compressed_sd[f\"vocab_layer_norm.{w}\"] = state_dict[\n",
    "        #         f\"cls.predictions.transform.LayerNorm.{w}\"\n",
    "        #     ]\n",
    "\n",
    "        return compressed_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"tuner007/pegasus_paraphrase\"\n",
    "# teacher = PegasusForMLM(model_name)\n",
    "# student = DistilpegasusStudentModel(model_name)\n",
    "\n",
    "teacher = torch.load('pg.pt')\n",
    "student = torch.load('st_3dec_3enc.pt')\n",
    "\n",
    "model = torch.nn.ModuleDict({\"teacher\": teacher, \"student\": student})\n",
    "\n",
    "callbacks = {\n",
    "    \"masked_lm_loss\": MaskedLanguageModelCallback(),\n",
    "    \"mse_loss\": MSELossCallback(),\n",
    "    \"cosine_loss\": CosineLossCallback(),\n",
    "    \"kl_div_loss\": KLDivLossCallback(),\n",
    "    \"loss\": MetricAggregationCallback(\n",
    "        prefix=\"loss\",\n",
    "        mode=\"weighted_sum\",\n",
    "        metrics={\n",
    "            \"cosine_loss\": 1.0,\n",
    "            \"masked_lm_loss\": 1.0,\n",
    "            \"kl_div_loss\": 1.0,\n",
    "            \"mse_loss\": 1.0\n",
    "        }\n",
    "    ),\n",
    "    \"optimizer\": dl.OptimizerCallback(),\n",
    "    \"perplexity\": PerplexityMetricCallbackDistillation()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ormer.layer.5.ffn.lin2.bias', 'pegasus_student.transformer.layer.5.output_layer_norm.bias', 'pegasus_student.transformer.layer.6.attention.q_lin.weight', 'pegasus_student.transformer.layer.6.attention.k_lin.weight', 'pegasus_student.transformer.layer.6.attention.v_lin.weight', 'pegasus_student.transformer.layer.6.attention.out_lin.weight', 'pegasus_student.transformer.layer.6.sa_layer_norm.weight', 'pegasus_student.transformer.layer.6.ffn.lin1.weight', 'pegasus_student.transformer.layer.6.ffn.lin2.weight', 'pegasus_student.transformer.layer.6.output_layer_norm.weight', 'pegasus_student.transformer.layer.6.attention.q_lin.bias', 'pegasus_student.transformer.layer.6.attention.k_lin.bias', 'pegasus_student.transformer.layer.6.attention.v_lin.bias', 'pegasus_student.transformer.layer.6.attention.out_lin.bias', 'pegasus_student.transformer.layer.6.sa_layer_norm.bias', 'pegasus_student.transformer.layer.6.ffn.lin1.bias', 'pegasus_student.transformer.layer.6.ffn.lin2.bias', 'pegasus_student.transformer.layer.6.output_layer_norm.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing PegasusForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PegasusForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PegasusForCausalLM were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.k_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.fc1.weight', 'decoder.layers.6.fc1.bias', 'decoder.layers.6.fc2.weight', 'decoder.layers.6.fc2.bias', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.k_proj.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.fc1.weight', 'decoder.layers.7.fc1.bias', 'decoder.layers.7.fc2.weight', 'decoder.layers.7.fc2.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.7.final_layer_norm.bias', 'decoder.layers.8.self_attn.k_proj.weight', 'decoder.layers.8.self_attn.k_proj.bias', 'decoder.layers.8.self_attn.v_proj.weight', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.8.self_attn.out_proj.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'decoder.layers.8.self_attn_layer_norm.weight', 'decoder.layers.8.self_attn_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.fc1.weight', 'decoder.layers.8.fc1.bias', 'decoder.layers.8.fc2.weight', 'decoder.layers.8.fc2.bias', 'decoder.layers.8.final_layer_norm.weight', 'decoder.layers.8.final_layer_norm.bias', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.9.self_attn.k_proj.bias', 'decoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.9.self_attn_layer_norm.weight', 'decoder.layers.9.self_attn_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.fc1.weight', 'decoder.layers.9.fc1.bias', 'decoder.layers.9.fc2.weight', 'decoder.layers.9.fc2.bias', 'decoder.layers.9.final_layer_norm.weight', 'decoder.layers.9.final_layer_norm.bias', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.10.self_attn.k_proj.bias', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.10.self_attn.out_proj.bias', 'decoder.layers.10.self_attn_layer_norm.weight', 'decoder.layers.10.self_attn_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.fc1.weight', 'decoder.layers.10.fc1.bias', 'decoder.layers.10.fc2.weight', 'decoder.layers.10.fc2.bias', 'decoder.layers.10.final_layer_norm.weight', 'decoder.layers.10.final_layer_norm.bias', 'decoder.layers.11.self_attn.k_proj.weight', 'decoder.layers.11.self_attn.k_proj.bias', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.11.self_attn_layer_norm.weight', 'decoder.layers.11.self_attn_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.fc1.weight', 'decoder.layers.11.fc1.bias', 'decoder.layers.11.fc2.weight', 'decoder.layers.11.fc2.bias', 'decoder.layers.11.final_layer_norm.weight', 'decoder.layers.11.final_layer_norm.bias', 'decoder.layers.12.self_attn.k_proj.weight', 'decoder.layers.12.self_attn.k_proj.bias', 'decoder.layers.12.self_attn.v_proj.weight', 'decoder.layers.12.self_attn.v_proj.bias', 'decoder.layers.12.self_attn.q_proj.weight', 'decoder.layers.12.self_attn.q_proj.bias', 'decoder.layers.12.self_attn.out_proj.weight', 'decoder.layers.12.self_attn.out_proj.bias', 'decoder.layers.12.self_attn_layer_norm.weight', 'decoder.layers.12.self_attn_layer_norm.bias', 'decoder.layers.12.encoder_attn.k_proj.weight', 'decoder.layers.12.encoder_attn.k_proj.bias', 'decoder.layers.12.encoder_attn.v_proj.weight', 'decoder.layers.12.encoder_attn.v_proj.bias', 'decoder.layers.12.encoder_attn.q_proj.weight', 'decoder.layers.12.encoder_attn.q_proj.bias', 'decoder.layers.12.encoder_attn.out_proj.weight', 'decoder.layers.12.encoder_attn.out_proj.bias', 'decoder.layers.12.encoder_attn_layer_norm.weight', 'decoder.layers.12.encoder_attn_layer_norm.bias', 'decoder.layers.12.fc1.weight', 'decoder.layers.12.fc1.bias', 'decoder.layers.12.fc2.weight', 'decoder.layers.12.fc2.bias', 'decoder.layers.12.final_layer_norm.weight', 'decoder.layers.12.final_layer_norm.bias', 'decoder.layers.13.self_attn.k_proj.weight', 'decoder.layers.13.self_attn.k_proj.bias', 'decoder.layers.13.self_attn.v_proj.weight', 'decoder.layers.13.self_attn.v_proj.bias', 'decoder.layers.13.self_attn.q_proj.weight', 'decoder.layers.13.self_attn.q_proj.bias', 'decoder.layers.13.self_attn.out_proj.weight', 'decoder.layers.13.self_attn.out_proj.bias', 'decoder.layers.13.self_attn_layer_norm.weight', 'decoder.layers.13.self_attn_layer_norm.bias', 'decoder.layers.13.encoder_attn.k_proj.weight', 'decoder.layers.13.encoder_attn.k_proj.bias', 'decoder.layers.13.encoder_attn.v_proj.weight', 'decoder.layers.13.encoder_attn.v_proj.bias', 'decoder.layers.13.encoder_attn.q_proj.weight', 'decoder.layers.13.encoder_attn.q_proj.bias', 'decoder.layers.13.encoder_attn.out_proj.weight', 'decoder.layers.13.encoder_attn.out_proj.bias', 'decoder.layers.13.encoder_attn_layer_norm.weight', 'decoder.layers.13.encoder_attn_layer_norm.bias', 'decoder.layers.13.fc1.weight', 'decoder.layers.13.fc1.bias', 'decoder.layers.13.fc2.weight', 'decoder.layers.13.fc2.bias', 'decoder.layers.13.final_layer_norm.weight', 'decoder.layers.13.final_layer_norm.bias', 'decoder.layers.14.self_attn.k_proj.weight', 'decoder.layers.14.self_attn.k_proj.bias', 'decoder.layers.14.self_attn.v_proj.weight', 'decoder.layers.14.self_attn.v_proj.bias', 'decoder.layers.14.self_attn.q_proj.weight', 'decoder.layers.14.self_attn.q_proj.bias', 'decoder.layers.14.self_attn.out_proj.weight', 'decoder.layers.14.self_attn.out_proj.bias', 'decoder.layers.14.self_attn_layer_norm.weight', 'decoder.layers.14.self_attn_layer_norm.bias', 'decoder.layers.14.encoder_attn.k_proj.weight', 'decoder.layers.14.encoder_attn.k_proj.bias', 'decoder.layers.14.encoder_attn.v_proj.weight', 'decoder.layers.14.encoder_attn.v_proj.bias', 'decoder.layers.14.encoder_attn.q_proj.weight', 'decoder.layers.14.encoder_attn.q_proj.bias', 'decoder.layers.14.encoder_attn.out_proj.weight', 'decoder.layers.14.encoder_attn.out_proj.bias', 'decoder.layers.14.encoder_attn_layer_norm.weight', 'decoder.layers.14.encoder_attn_layer_norm.bias', 'decoder.layers.14.fc1.weight', 'decoder.layers.14.fc1.bias', 'decoder.layers.14.fc2.weight', 'decoder.layers.14.fc2.bias', 'decoder.layers.14.final_layer_norm.weight', 'decoder.layers.14.final_layer_norm.bias', 'decoder.layers.15.self_attn.k_proj.weight', 'decoder.layers.15.self_attn.k_proj.bias', 'decoder.layers.15.self_attn.v_proj.weight', 'decoder.layers.15.self_attn.v_proj.bias', 'decoder.layers.15.self_attn.q_proj.weight', 'decoder.layers.15.self_attn.q_proj.bias', 'decoder.layers.15.self_attn.out_proj.weight', 'decoder.layers.15.self_attn.out_proj.bias', 'decoder.layers.15.self_attn_layer_norm.weight', 'decoder.layers.15.self_attn_layer_norm.bias', 'decoder.layers.15.encoder_attn.k_proj.weight', 'decoder.layers.15.encoder_attn.k_proj.bias', 'decoder.layers.15.encoder_attn.v_proj.weight', 'decoder.layers.15.encoder_attn.v_proj.bias', 'decoder.layers.15.encoder_attn.q_proj.weight', 'decoder.layers.15.encoder_attn.q_proj.bias', 'decoder.layers.15.encoder_attn.out_proj.weight', 'decoder.layers.15.encoder_attn.out_proj.bias', 'decoder.layers.15.encoder_attn_layer_norm.weight', 'decoder.layers.15.encoder_attn_layer_norm.bias', 'decoder.layers.15.fc1.weight', 'decoder.layers.15.fc1.bias', 'decoder.layers.15.fc2.weight', 'decoder.layers.15.fc2.bias', 'decoder.layers.15.final_layer_norm.weight', 'decoder.layers.15.final_layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layer_norm.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "teacher = PegasusForMLM(model_name)\n",
    "student = DistilpegasusStudentModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'teacher' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8245234920b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torch.save(teacher, 'pg.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mteacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pg.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pg_student.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'teacher' is not defined"
     ]
    }
   ],
   "source": [
    "# torch.save(teacher, 'pg.pt')\n",
    "teacher = torch.load(teacher, 'pg.pt')\n",
    "# torch.save(student, 'pg_student.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = torch.load('pg.pt')\n",
    "student = torch.load('st_3dec_3enc.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(\"train_dataset1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    }
   ],
   "source": [
    "batch = train_dataset['input_ids'][0]\n",
    "attenion_mask = train_dataset[0].attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e0981c6b9ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattenion_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, encoder_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         attention_mask = self._prepare_decoder_attention_mask(\n\u001b[0m\u001b[1;32m    949\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m_prepare_decoder_attention_mask\u001b[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0;31m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mexpanded_attn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expand_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m             combined_attention_mask = (\n\u001b[1;32m    841\u001b[0m                 \u001b[0mexpanded_attn_mask\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcombined_attention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mexpanded_attn_mask\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcombined_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m_expand_mask\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mExpands\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mtgt_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtgt_len\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "teacher.bert.model(torch.reshape(torch.tensor(batch), (1,130)), torch.tensor(attenion_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PegasusDecoderWrapper(\n",
       "  (decoder): PegasusDecoder(\n",
       "    (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "    (embed_positions): PegasusSinusoidalPositionalEmbedding(60, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): PegasusDecoderLayer(\n",
       "        (self_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): PegasusAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "teacher.bert.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([130])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "torch.tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[14937,  1034,   116,   582,   728, 10493, 47026,   141,   114,  1093,\n",
       "          1768,   107, 73032,  1722,  2729,   233,  1191, 23887,   121,  1768,\n",
       "           107, 73032,  1722,   214,   233,   551, 23887,   121,   115,   109,\n",
       "         61118,  2349,   640,   112, 19350, 12906,  1068,   110,   108,  3432,\n",
       "          1291, 42993,  2375,   110,   107,     1,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "torch.reshape(torch.tensor(batch), (1,130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.current_device())\n",
    "# print(torch.cuda.device(0))\n",
    "# print()\n",
    "state_dict = student.state_dict()\n",
    "f = open('state_dict_student.txt', 'a')\n",
    "f.write(','.join(map(str, student.state_dict())))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "1/1 * Epoch (train):   0% 0/85711 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "output_type": "error",
     "ename": "StopIteration",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d54777cc85d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilMLMRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m runner.train(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/dl/runner/runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, criterion, optimizer, scheduler, datasets, loaders, callbacks, logdir, resume, num_epochs, valid_loader, main_metric, minimize_metric, verbose, stage_kwargs, checkpoint_data, fp16, distributed, check, timeit, load_best_on_end, initial_seed, state_kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_cmd_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     def infer(\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/utils/scripts.py\u001b[0m in \u001b[0;36mdistributed_cmd_run\u001b[0;34m(worker_fn, distributed, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mor\u001b[0m \u001b[0mworld_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     ):\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mworker_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlocal_rank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/core/runner.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(self, experiment)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_exception_handler_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_exception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/core/runner.py\u001b[0m in \u001b[0;36m_run_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \"\"\"\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     def _batch2device(\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/core/callbacks/exception.py\u001b[0m in \u001b[0;36mon_exception\u001b[0;34m(self, runner)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneed_exception_reraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/core/runner.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(self, experiment)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExceptionCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/core/runner.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    896\u001b[0m             )\n\u001b[1;32m    897\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_epoch_start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/core/runner.py\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(self, stage, epoch)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_loader_start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_loader_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/core/runner.py\u001b[0m in \u001b[0;36m_run_loader\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_batch_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneed_early_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneed_early_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hug/lib/python3.8/site-packages/catalyst/core/runner.py\u001b[0m in \u001b[0;36m_run_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \"\"\"\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner = DistilMLMRunner(device=torch.device(\"cuda\"))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    verbose=True,\n",
    "    check=True,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning:\n\n`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'empty'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-12ec7b2ecf08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache() \n",
    "torch.cuda.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ea9b1223f9f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tuner007/pegasus_paraphrase\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'longest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'longest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk('cnn_dataset')\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "train_dataset = tokenizer(dataset['train']['article'],padding='longest')\n",
    "valid_dataset = tokenizer(dataset['validation']['article'],padding='longest')\n",
    "train_label = tokenizer(dataset['train']['highlights'],padding='longest')\n",
    "valid_label = tokenizer(dataset['validation']['highlights'],padding='longest')\n",
    "torch.save(train_dataset,'train_dataset1.pt')\n",
    "torch.save(valid_dataset,'valid_dataset1.pt')\n",
    "torch.save(train_label,'train_label1.pt')\n",
    "torch.save(valid_label,'valid_label1.pt')\n",
    "# train_dataset = LanguageModelingDataset(dataset['train']['article'][0:10000], tokenizer)\n",
    "# valid_dataset = LanguageModelingDataset(dataset['validation']['article'][0:10000], tokenizer)\n",
    "# train_label = LanguageModelingDataset(dataset['train']['highlights'][0:10000], tokenizer)\n",
    "# valid_label = LanguageModelingDataset(dataset['validation']['highlights'][0:10], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/numpy/core/_asarray.py:171: FutureWarning:\n",
      "\n",
      "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "/home/ibrahim/anaconda3/envs/hug/lib/python3.8/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.save('train_dataset1.npy',train_dataset)\n",
    "np.save('valid_dataset1.npy',valid_dataset)\n",
    "np.save('train_label1.npy',train_label)\n",
    "np.save('valid_label1.npy',valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([  143, 40155,   158,  1315, 12746,  1017,   112,   736,  2899,  1069,\n",
       "          694,   121, 18707,   428,   121,  6011, 36496,   112,  1348,  3637,\n",
       "          134,   109, 34372, 14367,   894,  3164,   107,  1969,   224,   109,\n",
       "         2808,   607,   112,   236,   199,   109, 36496,   138,   286,   107,\n",
       "            1])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9396"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "len(dataset['train']['article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 4.46kB [00:00, 3.86MB/s]                   \n",
      "Downloading: 2.16kB [00:00, 1.67MB/s]                   \n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset gigaword/default (download: 551.61 MiB, generated: 918.35 MiB, post-processed: Unknown size, total: 1.44 GiB) to /home/ibrahim/.cache/huggingface/datasets/gigaword/default/1.2.0/c518c578e42a6afe842b09e979ee2907ea42a12b57ba992fae9e9d7347825245...\n",
      "Downloading: 578MB [02:51, 3.38MB/s]\n",
      "Dataset gigaword downloaded and prepared to /home/ibrahim/.cache/huggingface/datasets/gigaword/default/1.2.0/c518c578e42a6afe842b09e979ee2907ea42a12b57ba992fae9e9d7347825245. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gigaword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('gigaword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38564bithugconda395815e3f2f24472803e292c993b0ade",
   "display_name": "Python 3.8.5 64-bit ('hug': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "a2b923c536f7b2ff6abff42f461db4a8f446db7981955f7a14121e5877ed5ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}