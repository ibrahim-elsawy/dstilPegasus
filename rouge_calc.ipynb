{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rouge_calc.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kkn_biMHnd3x",
        "outputId": "90b42bef-747b-4b81-bc08-42a65c0d24de"
      },
      "source": [
        "!nvidia-smi #### check it's T4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 14 13:51:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKZlDaQeHl8I",
        "outputId": "b643527d-e313-4ac6-a263-e862f06a4ed2"
      },
      "source": [
        "import os\n",
        "!git clone https://github.com/ibrahim-elsawy/dstilPegasus\n",
        "# !pip install -r /content/konafa/colab_req.txt\n",
        "!pip install datasets\n",
        "#!pip install torch==1.7.1\n",
        "!pip install transformers==4.3.0rc1\n",
        "!pip install sentencepiece \n",
        "!pip install catalyst==20.6\n",
        "!pip install rouge-score\n",
        "!pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dstilPegasus'...\n",
            "remote: Enumerating objects: 427, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 427 (delta 13), reused 2 (delta 2), pack-reused 400\u001b[K\n",
            "Receiving objects: 100% (427/427), 156.12 KiB | 4.73 MiB/s, done.\n",
            "Resolving deltas: 100% (271/271), done.\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/27/9c91ddee87b06d2de12f134c5171a49890427e398389f07f6463485723c3/datasets-1.9.0-py3-none-any.whl (262kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting fsspec>=2021.05.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/e1/7111d8afc76ee3171f4f99592cd29bac9d233ae1aa34623011506f955434/fsspec-2021.7.0-py3-none-any.whl (118kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: huggingface-hub, xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.9.0 fsspec-2021.7.0 huggingface-hub-0.0.13 xxhash-2.0.2\n",
            "Collecting transformers==4.3.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/e3/ee5acfce639c34466d7be3f31cb00e40fe13993e08f4161c718243b80441/transformers-4.3.0rc1-py3-none-any.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0rc1) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0rc1) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0rc1) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0rc1) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0rc1) (4.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0rc1) (3.0.12)\n",
            "Collecting tokenizers==0.10.1rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/72/3267f5946db7945bb50650b64abb09376ae15a872f4abe30c5812ff0a750/tokenizers-0.10.1rc1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0rc1) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.0rc1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.0rc1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.0rc1) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.0rc1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.0rc1) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.0rc1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.0rc1) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.0rc1) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.0rc1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.0rc1) (3.4.1)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.1rc1 transformers-4.3.0rc1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting catalyst==20.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/12/24661ef093301faa86950cb05f722db5cf669f8139a88cd3317616234200/catalyst-20.6-py2.py3-none-any.whl (368kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (1.19.5)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/0b/a26bbe92667c549d39c40b80c5ddec638fbae9521f04aeef26560e07e504/tensorboardX-2.4-py2.py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 14.4MB/s \n",
            "\u001b[?25hCollecting deprecation\n",
            "  Downloading https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl\n",
            "Collecting crc32c>=1.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/1c/b45d9351687bd3f1279c5b313b6aa83a4bdd55aae12aa767851206634804/crc32c-2.2.post0-cp37-cp37m-manylinux2010_x86_64.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (4.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (3.2.2)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (4.41.1)\n",
            "Collecting GitPython>=3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 16.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (2.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (20.9)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (3.13)\n",
            "Requirement already satisfied: pandas>=0.22 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (1.1.5)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (1.9.0+cu102)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (0.22.2.post1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from catalyst==20.6) (5.5.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->catalyst==20.6) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.1.0->catalyst==20.6) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.1.0->catalyst==20.6) (1.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catalyst==20.6) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catalyst==20.6) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catalyst==20.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catalyst==20.6) (0.10.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=3.1.1->catalyst==20.6) (3.7.4.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (0.36.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (1.32.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (1.34.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (57.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.6) (1.8.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22->catalyst==20.6) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->catalyst==20.6) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->catalyst==20.6) (1.0.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.6) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.6) (5.0.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.6) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.6) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.6) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.6) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.6) (1.0.18)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.6) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.6) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.6) (4.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14.0->catalyst==20.6) (4.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst==20.6) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.6) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.6) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.6) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.6) (1.24.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst==20.6) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->catalyst==20.6) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst==20.6) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.6) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14.0->catalyst==20.6) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst==20.6) (3.1.1)\n",
            "Installing collected packages: tensorboardX, deprecation, crc32c, smmap, gitdb, GitPython, catalyst\n",
            "Successfully installed GitPython-3.1.18 catalyst-20.6 crc32c-2.2.post0 deprecation-2.1.0 gitdb-4.0.7 smmap-4.0.0 tensorboardX-2.4\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hCollecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM4nUl0ErtQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe5b3e8-084a-4146-803f-b019ea06f969"
      },
      "source": [
        "os.chdir('/content/dstilPegasus')\n",
        "import torch\n",
        "import transformers\n",
        "from catalyst import dl\n",
        "from src.runners import DistilMLMRunner\n",
        "from src.runners import DistilMLMRunnerFT\n",
        "# from src.models import DistilpegasusStudentModel, PegasusForMLM #BertForMLM  DistilbertStudentModel\n",
        "from catalyst.core import MetricAggregationCallback\n",
        "from torch.utils.data import DataLoader\n",
        "from src.callbacks import (\n",
        "    CosineLossCallback,\n",
        "    KLDivLossCallback,\n",
        "    MaskedLanguageModelCallback,\n",
        "    MSELossCallback,\n",
        "    PerplexityMetricCallbackDistillation,\n",
        "    SmoothingLossCallback,\n",
        "    CrossentropylossCallback\n",
        ")\n",
        "from utils.rouge import calc_generative_metrics \n",
        "import pandas as pd\n",
        "from typing import Iterable, Union\n",
        "\n",
        "# import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "# import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
        "import numpy as np\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "from torch import nn\n",
        "from typing import Dict, List\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "from datasets import Dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "/usr/local/lib/python3.7/dist-packages/jsonschema/compat.py:6: DeprecationWarning:\n",
            "\n",
            "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/yaml/constructor.py:126: DeprecationWarning:\n",
            "\n",
            "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/decorators.py:70: DeprecationWarning:\n",
            "\n",
            "`formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQWqfFcWyj0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18277aba-4573-4e19-b588-aab59d1a7348"
      },
      "source": [
        "import os \n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I3ip3t8IFTv"
      },
      "source": [
        "# train_dataset = datasets.load_from_disk(\"/content/drive/MyDrive/pl/data/xsumtrainDataset_4\") ###train_dataset = datasets.load_from_disk(\"/content/drive/MyDrive/pl/data/xsumtrainDataset_2\")\n",
        "# valid_dataset = datasets.load_from_disk(\"/content/drive/MyDrive/pl/data/xsumvalidDataset\")\n",
        "# # train_dataset = datasets.load_from_disk(\"/content/drive/MyDrive/orig/xsumtrainDataset\") ###train_dataset = datasets.load_from_disk(\"/content/drive/MyDrive/pl/data/xsumtrainDataset_2\")\n",
        "# # valid_dataset = datasets.load_from_disk(\"/content/drive/MyDrive/orig/xsumvalidDataset\")\n",
        "test_dataset = datasets.load_from_disk(\"/content/drive/MyDrive/orig/xsumtestDataset\")\n",
        "teacher = torch.load('teacher_16_4.pt', map_location=torch.device('cuda')) #torch.load('teacher_model.pt', map_location=torch.device('cuda'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StlQ76P62641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349d131d-843c-40c5-bb50-38d229dec9b0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 14 13:53:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0    30W /  70W |   2484MiB / 15109MiB |      2%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ksqhbEYnoWu"
      },
      "source": [
        "import datasets\n",
        "import warnings\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional, Tuple, List, Union\n",
        "from transformers import PegasusModel, PegasusConfig, PegasusForConditionalGeneration\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PreTrainedModel\n",
        "from transformers import SummarizationPipeline\n",
        "students_config_book = {\n",
        "    '2': PegasusConfig(encoder_layers=2, decoder_layers=2),\n",
        "    '4': PegasusConfig(encoder_layers=4, decoder_layers=4),\n",
        "    '6': PegasusConfig(encoder_layers=6, decoder_layers=6),\n",
        "    '8': PegasusConfig(encoder_layers=8, decoder_layers=8),\n",
        "    '10': PegasusConfig(encoder_layers=10, decoder_layers=10),\n",
        "    '12': PegasusConfig(encoder_layers=12, decoder_layers=12),\n",
        "    '16': PegasusConfig(encoder_layers=16, decoder_layers=16)\n",
        "}\n",
        "\n",
        "\n",
        "LAYERS_TO_COPY = {\n",
        "    16: {  # maps  num layers in student -> which teacher layers to copy\n",
        "        1: [0],\n",
        "        2: [0, 15],\n",
        "        3: [0, 8, 15],\n",
        "        4: [0, 5, 10, 15],\n",
        "        6: [0, 3, 6, 9, 12, 15],\n",
        "        8: [0, 2, 4, 6, 8, 10, 12, 15],\n",
        "        9: [0, 1, 3, 5, 7, 9, 11, 13, 15],\n",
        "        12: [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 13, 15],\n",
        "        16: list(range(16)),\n",
        "    },\n",
        "}\n",
        "LAYERS_TO_SUPERVISE = {\n",
        "    # maps  num layers in student -> which teacher layers to copy.\n",
        "    16: {1: [15], 4: [4, 9, 12, 15], 8: [1, 3, 5, 7, 9, 11, 13, 15]},\n",
        "}\n",
        "\n",
        "\n",
        "def copy_layers(src_layers: nn.ModuleList, dest_layers: nn.ModuleList, layers_to_copy) -> None:\n",
        "    layers_to_copy = nn.ModuleList([src_layers[i] for i in layers_to_copy])\n",
        "    assert len(dest_layers) == len(\n",
        "        layers_to_copy), f\"{len(dest_layers)} != {len(layers_to_copy)}\"\n",
        "    dest_layers.load_state_dict(layers_to_copy.state_dict())\n",
        "\n",
        "# Copied from transformers.models.bart.modeling_bart.shift_tokens_right\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n",
        "\n",
        "def pick_layers_to_copy(n_student, n_teacher):\n",
        "    try:\n",
        "        val = LAYERS_TO_COPY[n_teacher][n_student]\n",
        "        return val\n",
        "    except KeyError:\n",
        "        if n_student != n_teacher:\n",
        "            warnings.warn(\n",
        "                f\"no hardcoded layers to copy for teacher {n_teacher} -> student {n_student}, defaulting to first {n_student}\"\n",
        "            )\n",
        "        return list(range(n_student))\n",
        "\n",
        "\n",
        "def get_layers_to_supervise(n_student, n_teacher) -> List[int]:\n",
        "    \"\"\"Used or the --supervise_forward kwarg\"\"\"\n",
        "    if n_student > n_teacher:\n",
        "        raise ValueError(\n",
        "            f\"Cannot perform intermediate supervision for student {n_student} > teacher {n_teacher}\")\n",
        "    elif n_teacher == n_student:\n",
        "        return list(range(n_teacher))\n",
        "    elif n_student == 1:\n",
        "        return [n_teacher - 1]\n",
        "    else:\n",
        "        return LAYERS_TO_SUPERVISE[n_teacher][n_student]\n",
        "\n",
        "\n",
        "def create_student_with_configuration(teacher,\n",
        "                                      e=None,\n",
        "                                      d=None,\n",
        "                                      copy_first_teacher_layers = False,\n",
        "                                      save_path='./student'):\n",
        "\n",
        "    teacher.eval()\n",
        "    teacher_e, teacher_d = teacher.config.encoder_layers, teacher.config.decoder_layers\n",
        "    init_kwargs = teacher.config.to_diff_dict()\n",
        "    if e is None:\n",
        "        e = teacher_e\n",
        "    if d is None:\n",
        "        d = teacher_d\n",
        "    init_kwargs.update({\"encoder_layers\": e, \"decoder_layers\": d})\n",
        "    student_cfg = teacher.config_class(**init_kwargs)\n",
        "    student = AutoModelForSeq2SeqLM.from_config(student_cfg)\n",
        "    # Start by copying the full teacher state dict this will copy the first N teacher layers to the student.\n",
        "    info = student.load_state_dict(teacher.state_dict(), strict=False)\n",
        "    # every student key should have a teacher keys.\n",
        "    assert info.missing_keys == [], info.missing_keys\n",
        "\n",
        "    if copy_first_teacher_layers:  # Our copying is done. We just log and save\n",
        "        e_layers_to_copy, d_layers_to_copy = list(range(e)), list(range(d))\n",
        "        student.save_pretrained(save_path)\n",
        "        return student, e_layers_to_copy, d_layers_to_copy\n",
        "\n",
        "    # Decide which layers of the teacher to copy. Not exactly alternating -- we try to keep first and last layer.\n",
        "    e_layers_to_copy: List[int] = pick_layers_to_copy(e, teacher_e)\n",
        "    d_layers_to_copy: List[int] = pick_layers_to_copy(d, teacher_d)\n",
        "\n",
        "    copy_layers(teacher.model.encoder.layers,\n",
        "                student.model.encoder.layers, e_layers_to_copy)\n",
        "    copy_layers(teacher.model.decoder.layers,\n",
        "                student.model.decoder.layers, d_layers_to_copy)\n",
        "\n",
        "    student.config.init_metadata = dict(\n",
        "        teacher_type=teacher.config.model_type,\n",
        "        copied_encoder_layers=e_layers_to_copy,\n",
        "        copied_decoder_layers=d_layers_to_copy,\n",
        "    )\n",
        "    #student.save_pretrained(save_path)\n",
        "    # Save information about copying for easier reproducibility\n",
        "\n",
        "    return student\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRhhzlI2nq6Z"
      },
      "source": [
        "student = create_student_with_configuration(teacher,\n",
        "                                     e=6,\n",
        "                                     d=4,\n",
        "                                     copy_first_teacher_layers = False,\n",
        "                                     save_path='./student').to('cuda')\n",
        "# student = teacher\n",
        "model = torch.nn.ModuleDict({\"student\": student})\n",
        "checkpoint = torch.load('trained_student_6_4_noTA.pt', map_location=torch.device('cuda'))\n",
        "student.load_state_dict(checkpoint['model_state_dict'])\n",
        "del teacher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0v6K5hb5ePy",
        "outputId": "82cb7e92-2147-403c-d63a-e00aecc6404e"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 14 06:45:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0    29W /  70W |   3642MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAbsNLYlt6ne",
        "outputId": "96e1e4d8-e21a-4602-d3a7-f3b2be5d3493"
      },
      "source": [
        "del checkpoint\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 14 06:45:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0    32W /  70W |   2710MiB / 15109MiB |     17%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkY1haJzR86"
      },
      "source": [
        "#train_dataset.set_format('torch', columns=['input_ids', 'attention_mask'], device='cuda')\n",
        "#valid_dataset.set_format('torch', columns=['input_ids', 'attention_mask'], device='cuda')\n",
        "test_data = Dataset.from_dict(test_dataset[0:])\n",
        "\n",
        "test_data.set_format('torch', columns=['input_ids', 'attention_mask','decode_ids','decode_mask'], device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8biq8M6ksICH"
      },
      "source": [
        "# train_dataloader = DataLoader(\n",
        "#     #train_dataset['train']\n",
        "#     train_data, batch_size=8\n",
        "# )\n",
        "test_dataloader = DataLoader(\n",
        "    test_data, batch_size=1\n",
        ")\n",
        "# loaders = {\"train\": train_dataloader, \"valid\": valid_dataloader}\n",
        "# del train_dataset\n",
        "# del valid_dataset\n",
        "#teacher = torch.load('teacher_model.pt', map_location=torch.device('cuda')).to('cuda')\n",
        "#student = torch.load('st_3dec_3enc.pt', map_location=torch.device('cuda')).to('cuda')\n",
        "#model = torch.nn.ModuleDict({\"teacher\": teacher, \"student\": student})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FwR-SRmKGM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45ba5c6-c4b3-49fa-eb1d-d5886be66842"
      },
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "# dict_scores = {\"rouge1\":[], \"rouge2\":[], \"rougeL\":[], \"rougeLsum\":[],\"all_time\" : []}\n",
        "pred = []\n",
        "labels = []\n",
        "inf_time = []\n",
        "#a_file = open(\"rouge_scores_1.pkl\", \"rb\")\n",
        "#dict_scores = pickle.load(a_file)\n",
        "#a_file.close()\n",
        "student.eval()\n",
        "for i,batch in enumerate(test_dataloader):\n",
        "  batch['decode_ids'] = batch['decode_ids'].to('cuda')\n",
        "  batch['input_ids'] = batch['input_ids'].to('cuda')\n",
        "  batch['attention_mask'] = batch['attention_mask'].to('cuda')\n",
        "  labels.append(tokenizer.batch_decode(batch['decode_ids'],skip_special_tokens=True)[0])\n",
        "  time_before = time.time()\n",
        "  pred_ids = student.generate(input_ids = batch['input_ids'],attention_mask = batch['attention_mask'],max_length=60,use_cache=True )\n",
        "  time_after = time.time()\n",
        "  pred.append(tokenizer.batch_decode(pred_ids,skip_special_tokens=True)[0])\n",
        "  inf_time.append((time_after-time_before)/batch[\"input_ids\"].shape[0])\n",
        "  if i%10 == 0:\n",
        "    print(i)\n",
        "scores = calc_generative_metrics(pred,labels)\n",
        "print(scores)\n",
        "print(np.average(inf_time[:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning:\n",
            "\n",
            "floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n",
            "380\n",
            "390\n",
            "400\n",
            "410\n",
            "420\n",
            "430\n",
            "440\n",
            "450\n",
            "460\n",
            "470\n",
            "480\n",
            "490\n",
            "500\n",
            "510\n",
            "520\n",
            "530\n",
            "540\n",
            "550\n",
            "560\n",
            "570\n",
            "580\n",
            "590\n",
            "600\n",
            "610\n",
            "620\n",
            "630\n",
            "640\n",
            "650\n",
            "660\n",
            "670\n",
            "680\n",
            "690\n",
            "700\n",
            "710\n",
            "720\n",
            "730\n",
            "740\n",
            "750\n",
            "760\n",
            "770\n",
            "780\n",
            "790\n",
            "800\n",
            "810\n",
            "820\n",
            "830\n",
            "840\n",
            "850\n",
            "860\n",
            "870\n",
            "880\n",
            "890\n",
            "900\n",
            "910\n",
            "920\n",
            "930\n",
            "940\n",
            "950\n",
            "960\n",
            "970\n",
            "980\n",
            "990\n",
            "1000\n",
            "1010\n",
            "1020\n",
            "1030\n",
            "1040\n",
            "1050\n",
            "1060\n",
            "1070\n",
            "1080\n",
            "1090\n",
            "1100\n",
            "1110\n",
            "1120\n",
            "1130\n",
            "1140\n",
            "1150\n",
            "1160\n",
            "1170\n",
            "1180\n",
            "1190\n",
            "1200\n",
            "1210\n",
            "1220\n",
            "1230\n",
            "1240\n",
            "1250\n",
            "1260\n",
            "1270\n",
            "1280\n",
            "1290\n",
            "1300\n",
            "1310\n",
            "1320\n",
            "1330\n",
            "1340\n",
            "1350\n",
            "1360\n",
            "1370\n",
            "1380\n",
            "1390\n",
            "1400\n",
            "1410\n",
            "1420\n",
            "1430\n",
            "1440\n",
            "1450\n",
            "1460\n",
            "1470\n",
            "1480\n",
            "1490\n",
            "1500\n",
            "1510\n",
            "1520\n",
            "1530\n",
            "1540\n",
            "1550\n",
            "1560\n",
            "1570\n",
            "1580\n",
            "1590\n",
            "1600\n",
            "1610\n",
            "1620\n",
            "1630\n",
            "1640\n",
            "1650\n",
            "1660\n",
            "1670\n",
            "1680\n",
            "1690\n",
            "1700\n",
            "1710\n",
            "1720\n",
            "1730\n",
            "1740\n",
            "1750\n",
            "1760\n",
            "1770\n",
            "1780\n",
            "1790\n",
            "1800\n",
            "1810\n",
            "1820\n",
            "1830\n",
            "1840\n",
            "1850\n",
            "1860\n",
            "1870\n",
            "1880\n",
            "1890\n",
            "1900\n",
            "1910\n",
            "1920\n",
            "1930\n",
            "1940\n",
            "1950\n",
            "1960\n",
            "1970\n",
            "1980\n",
            "1990\n",
            "2000\n",
            "2010\n",
            "2020\n",
            "2030\n",
            "2040\n",
            "2050\n",
            "2060\n",
            "2070\n",
            "2080\n",
            "2090\n",
            "2100\n",
            "2110\n",
            "2120\n",
            "2130\n",
            "2140\n",
            "2150\n",
            "2160\n",
            "2170\n",
            "2180\n",
            "2190\n",
            "2200\n",
            "2210\n",
            "2220\n",
            "2230\n",
            "2240\n",
            "2250\n",
            "2260\n",
            "2270\n",
            "2280\n",
            "2290\n",
            "2300\n",
            "2310\n",
            "2320\n",
            "2330\n",
            "2340\n",
            "2350\n",
            "2360\n",
            "2370\n",
            "2380\n",
            "2390\n",
            "2400\n",
            "2410\n",
            "2420\n",
            "2430\n",
            "2440\n",
            "2450\n",
            "2460\n",
            "2470\n",
            "2480\n",
            "2490\n",
            "2500\n",
            "2510\n",
            "2520\n",
            "2530\n",
            "2540\n",
            "2550\n",
            "2560\n",
            "2570\n",
            "2580\n",
            "2590\n",
            "2600\n",
            "2610\n",
            "2620\n",
            "2630\n",
            "2640\n",
            "2650\n",
            "2660\n",
            "2670\n",
            "2680\n",
            "2690\n",
            "2700\n",
            "2710\n",
            "2720\n",
            "2730\n",
            "2740\n",
            "2750\n",
            "2760\n",
            "2770\n",
            "2780\n",
            "2790\n",
            "2800\n",
            "2810\n",
            "2820\n",
            "2830\n",
            "2840\n",
            "2850\n",
            "2860\n",
            "2870\n",
            "2880\n",
            "2890\n",
            "2900\n",
            "2910\n",
            "2920\n",
            "2930\n",
            "2940\n",
            "2950\n",
            "2960\n",
            "2970\n",
            "2980\n",
            "2990\n",
            "3000\n",
            "3010\n",
            "3020\n",
            "3030\n",
            "3040\n",
            "3050\n",
            "3060\n",
            "3070\n",
            "3080\n",
            "3090\n",
            "3100\n",
            "3110\n",
            "3120\n",
            "3130\n",
            "3140\n",
            "3150\n",
            "3160\n",
            "3170\n",
            "3180\n",
            "3190\n",
            "3200\n",
            "3210\n",
            "3220\n",
            "3230\n",
            "3240\n",
            "3250\n",
            "3260\n",
            "3270\n",
            "3280\n",
            "3290\n",
            "3300\n",
            "3310\n",
            "3320\n",
            "3330\n",
            "3340\n",
            "3350\n",
            "3360\n",
            "3370\n",
            "3380\n",
            "3390\n",
            "3400\n",
            "3410\n",
            "3420\n",
            "3430\n",
            "3440\n",
            "3450\n",
            "3460\n",
            "3470\n",
            "3480\n",
            "3490\n",
            "3500\n",
            "3510\n",
            "3520\n",
            "3530\n",
            "3540\n",
            "3550\n",
            "3560\n",
            "3570\n",
            "3580\n",
            "3590\n",
            "3600\n",
            "3610\n",
            "3620\n",
            "3630\n",
            "3640\n",
            "3650\n",
            "3660\n",
            "3670\n",
            "3680\n",
            "3690\n",
            "3700\n",
            "3710\n",
            "3720\n",
            "3730\n",
            "3740\n",
            "3750\n",
            "3760\n",
            "3770\n",
            "3780\n",
            "3790\n",
            "3800\n",
            "3810\n",
            "3820\n",
            "3830\n",
            "3840\n",
            "3850\n",
            "3860\n",
            "3870\n",
            "3880\n",
            "3890\n",
            "3900\n",
            "3910\n",
            "3920\n",
            "3930\n",
            "3940\n",
            "3950\n",
            "3960\n",
            "3970\n",
            "3980\n",
            "3990\n",
            "4000\n",
            "4010\n",
            "4020\n",
            "4030\n",
            "4040\n",
            "4050\n",
            "4060\n",
            "4070\n",
            "4080\n",
            "4090\n",
            "4100\n",
            "4110\n",
            "4120\n",
            "4130\n",
            "4140\n",
            "4150\n",
            "4160\n",
            "4170\n",
            "4180\n",
            "4190\n",
            "4200\n",
            "4210\n",
            "4220\n",
            "4230\n",
            "4240\n",
            "4250\n",
            "4260\n",
            "4270\n",
            "4280\n",
            "4290\n",
            "4300\n",
            "4310\n",
            "4320\n",
            "4330\n",
            "4340\n",
            "4350\n",
            "4360\n",
            "4370\n",
            "4380\n",
            "4390\n",
            "4400\n",
            "4410\n",
            "4420\n",
            "4430\n",
            "4440\n",
            "4450\n",
            "4460\n",
            "4470\n",
            "4480\n",
            "4490\n",
            "4500\n",
            "4510\n",
            "4520\n",
            "4530\n",
            "4540\n",
            "4550\n",
            "4560\n",
            "4570\n",
            "4580\n",
            "4590\n",
            "4600\n",
            "4610\n",
            "4620\n",
            "4630\n",
            "4640\n",
            "4650\n",
            "4660\n",
            "4670\n",
            "4680\n",
            "4690\n",
            "4700\n",
            "4710\n",
            "4720\n",
            "4730\n",
            "4740\n",
            "4750\n",
            "4760\n",
            "4770\n",
            "4780\n",
            "4790\n",
            "4800\n",
            "4810\n",
            "4820\n",
            "4830\n",
            "4840\n",
            "4850\n",
            "4860\n",
            "4870\n",
            "4880\n",
            "4890\n",
            "4900\n",
            "4910\n",
            "4920\n",
            "4930\n",
            "4940\n",
            "4950\n",
            "4960\n",
            "4970\n",
            "4980\n",
            "4990\n",
            "5000\n",
            "5010\n",
            "5020\n",
            "5030\n",
            "5040\n",
            "5050\n",
            "5060\n",
            "5070\n",
            "5080\n",
            "5090\n",
            "5100\n",
            "5110\n",
            "5120\n",
            "5130\n",
            "5140\n",
            "5150\n",
            "5160\n",
            "5170\n",
            "5180\n",
            "5190\n",
            "5200\n",
            "5210\n",
            "5220\n",
            "5230\n",
            "5240\n",
            "5250\n",
            "5260\n",
            "5270\n",
            "5280\n",
            "5290\n",
            "5300\n",
            "5310\n",
            "5320\n",
            "5330\n",
            "5340\n",
            "5350\n",
            "5360\n",
            "5370\n",
            "5380\n",
            "5390\n",
            "5400\n",
            "5410\n",
            "5420\n",
            "5430\n",
            "5440\n",
            "5450\n",
            "5460\n",
            "5470\n",
            "5480\n",
            "5490\n",
            "5500\n",
            "5510\n",
            "5520\n",
            "5530\n",
            "5540\n",
            "5550\n",
            "5560\n",
            "5570\n",
            "5580\n",
            "5590\n",
            "5600\n",
            "5610\n",
            "5620\n",
            "5630\n",
            "5640\n",
            "5650\n",
            "5660\n",
            "5670\n",
            "5680\n",
            "5690\n",
            "5700\n",
            "5710\n",
            "5720\n",
            "5730\n",
            "5740\n",
            "5750\n",
            "5760\n",
            "5770\n",
            "5780\n",
            "5790\n",
            "5800\n",
            "5810\n",
            "5820\n",
            "5830\n",
            "5840\n",
            "5850\n",
            "5860\n",
            "5870\n",
            "5880\n",
            "5890\n",
            "5900\n",
            "5910\n",
            "5920\n",
            "5930\n",
            "5940\n",
            "5950\n",
            "5960\n",
            "5970\n",
            "5980\n",
            "5990\n",
            "6000\n",
            "6010\n",
            "6020\n",
            "6030\n",
            "6040\n",
            "6050\n",
            "6060\n",
            "6070\n",
            "6080\n",
            "6090\n",
            "6100\n",
            "6110\n",
            "6120\n",
            "6130\n",
            "6140\n",
            "6150\n",
            "6160\n",
            "6170\n",
            "6180\n",
            "6190\n",
            "6200\n",
            "6210\n",
            "6220\n",
            "6230\n",
            "6240\n",
            "6250\n",
            "6260\n",
            "6270\n",
            "6280\n",
            "6290\n",
            "6300\n",
            "6310\n",
            "6320\n",
            "6330\n",
            "6340\n",
            "6350\n",
            "6360\n",
            "6370\n",
            "6380\n",
            "6390\n",
            "6400\n",
            "6410\n",
            "6420\n",
            "6430\n",
            "6440\n",
            "6450\n",
            "6460\n",
            "6470\n",
            "6480\n",
            "6490\n",
            "6500\n",
            "6510\n",
            "6520\n",
            "6530\n",
            "6540\n",
            "6550\n",
            "6560\n",
            "6570\n",
            "6580\n",
            "6590\n",
            "6600\n",
            "6610\n",
            "6620\n",
            "6630\n",
            "6640\n",
            "6650\n",
            "6660\n",
            "6670\n",
            "6680\n",
            "6690\n",
            "6700\n",
            "6710\n",
            "6720\n",
            "6730\n",
            "6740\n",
            "6750\n",
            "6760\n",
            "6770\n",
            "6780\n",
            "6790\n",
            "6800\n",
            "6810\n",
            "6820\n",
            "6830\n",
            "6840\n",
            "6850\n",
            "6860\n",
            "6870\n",
            "6880\n",
            "6890\n",
            "6900\n",
            "6910\n",
            "6920\n",
            "6930\n",
            "6940\n",
            "6950\n",
            "6960\n",
            "6970\n",
            "6980\n",
            "6990\n",
            "7000\n",
            "7010\n",
            "7020\n",
            "7030\n",
            "7040\n",
            "7050\n",
            "7060\n",
            "7070\n",
            "7080\n",
            "7090\n",
            "7100\n",
            "7110\n",
            "7120\n",
            "7130\n",
            "7140\n",
            "7150\n",
            "7160\n",
            "7170\n",
            "7180\n",
            "7190\n",
            "7200\n",
            "7210\n",
            "7220\n",
            "7230\n",
            "7240\n",
            "7250\n",
            "7260\n",
            "7270\n",
            "7280\n",
            "7290\n",
            "7300\n",
            "7310\n",
            "7320\n",
            "7330\n",
            "7340\n",
            "7350\n",
            "7360\n",
            "7370\n",
            "7380\n",
            "7390\n",
            "7400\n",
            "7410\n",
            "7420\n",
            "7430\n",
            "7440\n",
            "7450\n",
            "7460\n",
            "7470\n",
            "7480\n",
            "7490\n",
            "7500\n",
            "7510\n",
            "7520\n",
            "7530\n",
            "7540\n",
            "7550\n",
            "7560\n",
            "7570\n",
            "7580\n",
            "7590\n",
            "7600\n",
            "7610\n",
            "7620\n",
            "7630\n",
            "7640\n",
            "7650\n",
            "7660\n",
            "7670\n",
            "7680\n",
            "7690\n",
            "7700\n",
            "7710\n",
            "7720\n",
            "7730\n",
            "7740\n",
            "7750\n",
            "7760\n",
            "7770\n",
            "7780\n",
            "7790\n",
            "7800\n",
            "7810\n",
            "7820\n",
            "7830\n",
            "7840\n",
            "7850\n",
            "7860\n",
            "7870\n",
            "7880\n",
            "7890\n",
            "7900\n",
            "7910\n",
            "7920\n",
            "7930\n",
            "7940\n",
            "7950\n",
            "7960\n",
            "7970\n",
            "7980\n",
            "7990\n",
            "8000\n",
            "8010\n",
            "8020\n",
            "8030\n",
            "8040\n",
            "8050\n",
            "8060\n",
            "8070\n",
            "8080\n",
            "8090\n",
            "8100\n",
            "8110\n",
            "8120\n",
            "8130\n",
            "8140\n",
            "8150\n",
            "8160\n",
            "8170\n",
            "8180\n",
            "8190\n",
            "8200\n",
            "8210\n",
            "8220\n",
            "8230\n",
            "8240\n",
            "8250\n",
            "8260\n",
            "8270\n",
            "8280\n",
            "8290\n",
            "8300\n",
            "8310\n",
            "8320\n",
            "8330\n",
            "8340\n",
            "8350\n",
            "8360\n",
            "8370\n",
            "8380\n",
            "8390\n",
            "8400\n",
            "8410\n",
            "8420\n",
            "8430\n",
            "8440\n",
            "8450\n",
            "8460\n",
            "8470\n",
            "8480\n",
            "8490\n",
            "8500\n",
            "8510\n",
            "8520\n",
            "8530\n",
            "8540\n",
            "8550\n",
            "8560\n",
            "8570\n",
            "8580\n",
            "8590\n",
            "8600\n",
            "8610\n",
            "8620\n",
            "8630\n",
            "8640\n",
            "8650\n",
            "8660\n",
            "8670\n",
            "8680\n",
            "8690\n",
            "8700\n",
            "8710\n",
            "8720\n",
            "8730\n",
            "8740\n",
            "8750\n",
            "8760\n",
            "8770\n",
            "8780\n",
            "8790\n",
            "8800\n",
            "8810\n",
            "8820\n",
            "8830\n",
            "8840\n",
            "8850\n",
            "8860\n",
            "8870\n",
            "8880\n",
            "8890\n",
            "8900\n",
            "8910\n",
            "8920\n",
            "8930\n",
            "8940\n",
            "8950\n",
            "8960\n",
            "8970\n",
            "8980\n",
            "8990\n",
            "9000\n",
            "9010\n",
            "9020\n",
            "9030\n",
            "9040\n",
            "9050\n",
            "9060\n",
            "9070\n",
            "9080\n",
            "9090\n",
            "9100\n",
            "9110\n",
            "9120\n",
            "9130\n",
            "9140\n",
            "9150\n",
            "9160\n",
            "9170\n",
            "9180\n",
            "9190\n",
            "9200\n",
            "9210\n",
            "9220\n",
            "9230\n",
            "9240\n",
            "9250\n",
            "9260\n",
            "9270\n",
            "9280\n",
            "9290\n",
            "9300\n",
            "9310\n",
            "9320\n",
            "9330\n",
            "9340\n",
            "9350\n",
            "9360\n",
            "9370\n",
            "9380\n",
            "9390\n",
            "9400\n",
            "9410\n",
            "9420\n",
            "9430\n",
            "9440\n",
            "9450\n",
            "9460\n",
            "9470\n",
            "9480\n",
            "9490\n",
            "9500\n",
            "9510\n",
            "9520\n",
            "9530\n",
            "9540\n",
            "9550\n",
            "9560\n",
            "9570\n",
            "9580\n",
            "9590\n",
            "9600\n",
            "9610\n",
            "9620\n",
            "9630\n",
            "9640\n",
            "9650\n",
            "9660\n",
            "9670\n",
            "9680\n",
            "9690\n",
            "9700\n",
            "9710\n",
            "9720\n",
            "9730\n",
            "9740\n",
            "9750\n",
            "9760\n",
            "9770\n",
            "9780\n",
            "9790\n",
            "9800\n",
            "9810\n",
            "9820\n",
            "9830\n",
            "9840\n",
            "9850\n",
            "9860\n",
            "9870\n",
            "9880\n",
            "9890\n",
            "9900\n",
            "9910\n",
            "9920\n",
            "9930\n",
            "9940\n",
            "9950\n",
            "9960\n",
            "9970\n",
            "9980\n",
            "9990\n",
            "10000\n",
            "10010\n",
            "10020\n",
            "10030\n",
            "10040\n",
            "10050\n",
            "10060\n",
            "10070\n",
            "10080\n",
            "10090\n",
            "10100\n",
            "10110\n",
            "10120\n",
            "10130\n",
            "10140\n",
            "10150\n",
            "10160\n",
            "10170\n",
            "10180\n",
            "10190\n",
            "10200\n",
            "10210\n",
            "10220\n",
            "10230\n",
            "10240\n",
            "10250\n",
            "10260\n",
            "10270\n",
            "10280\n",
            "10290\n",
            "10300\n",
            "10310\n",
            "10320\n",
            "10330\n",
            "10340\n",
            "10350\n",
            "10360\n",
            "10370\n",
            "10380\n",
            "10390\n",
            "10400\n",
            "10410\n",
            "10420\n",
            "10430\n",
            "10440\n",
            "10450\n",
            "10460\n",
            "10470\n",
            "10480\n",
            "10490\n",
            "10500\n",
            "10510\n",
            "10520\n",
            "10530\n",
            "10540\n",
            "10550\n",
            "10560\n",
            "10570\n",
            "10580\n",
            "10590\n",
            "10600\n",
            "10610\n",
            "10620\n",
            "10630\n",
            "10640\n",
            "10650\n",
            "10660\n",
            "10670\n",
            "10680\n",
            "10690\n",
            "10700\n",
            "10710\n",
            "10720\n",
            "10730\n",
            "10740\n",
            "10750\n",
            "10760\n",
            "10770\n",
            "10780\n",
            "10790\n",
            "10800\n",
            "10810\n",
            "10820\n",
            "10830\n",
            "10840\n",
            "10850\n",
            "10860\n",
            "10870\n",
            "10880\n",
            "10890\n",
            "10900\n",
            "10910\n",
            "10920\n",
            "10930\n",
            "10940\n",
            "10950\n",
            "10960\n",
            "10970\n",
            "10980\n",
            "10990\n",
            "11000\n",
            "11010\n",
            "11020\n",
            "11030\n",
            "11040\n",
            "11050\n",
            "11060\n",
            "11070\n",
            "11080\n",
            "11090\n",
            "11100\n",
            "11110\n",
            "11120\n",
            "11130\n",
            "11140\n",
            "11150\n",
            "11160\n",
            "11170\n",
            "11180\n",
            "11190\n",
            "11200\n",
            "11210\n",
            "11220\n",
            "11230\n",
            "11240\n",
            "11250\n",
            "11260\n",
            "11270\n",
            "11280\n",
            "11290\n",
            "11300\n",
            "11310\n",
            "11320\n",
            "11330\n",
            "{'rouge1': 44.2993, 'rouge2': 21.6741, 'rougeL': 36.8207, 'rougeLsum': 36.8298}\n",
            "0.5260205361627283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKmW_agKifwl"
      },
      "source": [
        "def get_response(input_text):\n",
        "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=1000, return_tensors=\"pt\").to('cuda')\n",
        "  translated = student.generate(**batch,max_length=60,use_cache=True ,num_return_sequences=1)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tseH_B-MlY3R",
        "outputId": "e0a435ee-fb21-4d60-a3a7-0e984dc2a995"
      },
      "source": [
        "context1 = \"The problem is affecting people using the older versions of the PlayStation 3, called the 'Fat' model.The problem isn't affecting the newer PS3 Slim systems that have been on sale since September last year.Sony have also said they are aiming to have the problem fixed shortly but is advising some users to avoid using their console for the time being.'We hope to resolve this problem within the next 24 hours,' a statement reads. 'In the meantime, if you have a model other than the new slim PS3, we advise that you do not use your PS3 system, as doing so may result in errors in some functionality, such as recording obtained trophies, and not being able to restore certain data.'We believe we have identified that this problem is being caused by a bug in the clock functionality incorporated in the system.'The PlayStation Network is used by millions of people around the world.It allows users to play their friends at games like Fifa over the internet and also do things like download software or visit online stores.\"\n",
        "print(get_response(context1))\n",
        "context2 = \"She will play Denker, a lady’s maid to Dame Maggie Smith’s character, the Dowager Countess of Grantham. Johnston, who has also appeared in Waking the Dead and Coronation Street, joins new stars Richard E Grant and Anna Chancellor, both of whom will play guests of the Granthams at Downton. The hit period drama will return to screens this autumn. Series four of the show, which followed the wealthy Grantham family and their servants, achieved an average of 11.9 million viewers in the UK. The very British drama has also been a huge hit in the US, winning both Emmy Awards and Golden Globes. More than 26 million viewers watched series four on Masterpiece on PBS, making it one of the highest rating shows on American television. Previous high profile guest stars include Shirley Maclaine who played Martha Levinson, Lady Grantham’s mother, and Oscar-nominated actor Paul Giamatti who appeared in last year’s Christmas special as her ”maverick, playboy” son. Series five will also feature 24 star Rade Sherbedgia as a Russian refugee who has fled the revolution after World War 1. Earlier this year, executive producer Gareth Neame promised it would have ”all the usual highs and lows, romance, drama and comedy”.\"\n",
        "print(get_response(context2))\n",
        "context3 = \"Media playback is not supported on this device Craig Cathcart put the visitors ahead before substitute Simon Church won and scored an 89th-minute penalty. ”There were lots of positives out of it even if we’d have come off and lost 1-0. They had a good mentality and attitude,” said Coleman. Wales face another Euro 2016 warm-up game against Ukraine in Kiev on Monday. ”We look forward to our next challenge now,” added Coleman. ”The team will change up again, and we’ll see how they go again.” Striker Church, currently on loan at Scottish Premiership side Aberdeen from Reading, was delighted with his equaliser from the spot. ”Northern Ireland were a tough side to play against. They’ve obviously done well to get where they are and it was a tough game,” he said. ”We wanted to do well because it was the last time a Wales crowd would see us before the Euros and we wanted to put in a good performance. ”I’ve just got to keep going now and hopefully score some goals. This is a great squad to be part of.”\"\n",
        "print(get_response(context3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sony has warned users that they are unable to use their PlayStation 3 console because of a bug in the clock.']\n",
            "['Actress Joanna Johnston has joined the cast of Downton Abbey in the fifth series of ITV’s Downton Abbey.']\n",
            "['Wales manager Chris Coleman praised his side’s attitude after they drew 1-1 with Northern Ireland in their final warm-up game before Euro 2016.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVk_2KNpTFk6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}